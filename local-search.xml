<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>神经网络-Cross Validation</title>
      <link href="undefined2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Cross-Validation/"/>
      <url>2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Cross-Validation/</url>
      
        <content type="html"><![CDATA[<p>当我们拿到一批数据，搭建了一个模型，准备为这个模型训练参数的时候，我们使用全部数据对模型计算损失和准确率，这会得到我们在整个训练集上的损失和准确率。这批数据是在我们训练过程中已经使用过的数据，那我们是如何判定这个模型在新的数据上的表现，是出色还是糟糕。</p><p>萌生这种想法，这种想法被称作交叉验证（Cross Validation）。我们拿到一批数据以后，并不是将这批数据全部都喂进我们的模型中，而是将这批数据做拆分，一批数据作为训练集，另外一批作为验证集。这样我们就能保证在验证集中获得的loss和acc是在未知的新数据中得到的。我们就可以以这个来评定我们的模型在新数据中表现的是出色还是糟糕。</p><h1 id="Shuffle-Data"><a href="#Shuffle-Data" class="headerlink" title="Shuffle Data"></a>Shuffle Data</h1><p>在拿到一批数据以后，有极大的可能这批数据集是有关联性的，比如是按照时间顺序排列，或者说前面部分是Class1 后面部分是Class2。这样在后面的数据集分离后，获得的数据集将会是不均很的，很可能在训练集当中70%都是Class1分类，而在验证集当中所有都是Class2分类。</p><p>为了避免这个问题的发生，我们的做法是，将数据集打乱，虽说不能完全避免这类情况的发生，但是极大降低了产生的几率。在本文中，仅将数据集切分划分为训练集和验证集，更权威的做法是将数据集分割成 训练集(train)、交叉验证集(cv)、测试集(test)，在训练集上训练我们的目标模型，在交叉验证集上找出我们较为好的模型（比如我们有多个多项式模型，需要寻找这个参数在几次方模型上表现的出色），最终在测试集上对刚才找出的模型做测试，得出在模型在新数据上的表现。具体操作做法在本文中不做讲解，可以参照下方的分割方法，大同小异</p><p>下面是我个人写的对数据集进行shuffle的代码，当然在现阶段的很多框架当中都有类似的方法可以直接拿来使用，比如sklearn中的shuffle。</p><pre><code class="python">import numpy as npp = np.random.permutation(x.shape[0])x = np.array(x)[p]y = np.array(y)[p]</code></pre><p>这样我们就获得了做了shuffle以后的data。就可以对其进行训练集验证集的划分。</p><h1 id="Simple交叉验证"><a href="#Simple交叉验证" class="headerlink" title="Simple交叉验证"></a>Simple交叉验证</h1><p>简单交叉验证就是单独将数据集做一次性划分，按照自己给定的比例将数据集做一个划分。通常我们将70%的数据划分作为训练集，30%的数据划分作为验证集。 训练集 &gt; 验证集。当然 Up to you.</p><pre><code class="python">if rate &gt;= 1 or rate &lt;=0 :    raise Exception(&quot;请设置rate为(0,1)之间的数&quot;)train_ind = int(x.shape[0] * rate)np.array(x[:train_ind]),np.array(x[train_ind+1:]),np.array(y[:train_ind]),np.array(y[train_ind+1:])</code></pre><p>所实现的更能和sklearn中的拆分类似，下面展示我的所有代码。</p><pre><code class="python">def shuffle_split(x, y , rate = 0.7):    &quot;&quot;&quot;    对数据进行shuffle，切分数据集为训练集和验证集    &quot;&quot;&quot;    if rate &gt;= 1 or rate &lt;=0 :        raise Exception(&quot;请设置rate为(0,1)之间的数&quot;)    p = np.random.permutation(x.shape[0])    x = np.array(x)[p]    y = np.array(y)[p]    train_ind = int(x.shape[0] * rate)    return np.array(x[:train_ind]),np.array(x[train_ind+1:]),np.array(y[:train_ind]),np.array(y[train_ind+1:])    ```# K-Fold 交叉验证K-Fild 数据集拆分K份，在每轮训练中，将使用不同的训练集：将随机使用K-1份作为训练集，单一的那份作为验证集，当该轮训练结束后需要重新获取选择新的K-1份数据集作为训练集。代码实现自己造轮子吧。当然也可使用各框架中的方法。Up to youBest wishes</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-Random Initialization</title>
      <link href="undefined2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Random-Initialization/"/>
      <url>2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Random-Initialization/</url>
      
        <content type="html"><![CDATA[<p>在机器学习的过程中参数必然需要进行初始化操作，我们具体把参数初始化为什么较好呢？是否可以将所有的权重向量设置初始化为同样的值，比如0？</p><p>答案是否定的，在吴恩达老师讲解的机器学习课程中，对于这一个初始化做了详细的讲解。</p><h1 id="Zero-initialization"><a href="#Zero-initialization" class="headerlink" title="Zero initialization"></a>Zero initialization</h1><p>将所有参数在初始化的时候初始化为0，我们可以试着这么理解，在前端输入层输入了相同的数据，我们根据这个均相同的参数权重去计算下一层的输入。这就意味这我们会以相同的输入函数去计算，对每个输入的sample去训练，这样来说我们下一层的所有输入将会是相同的，这样一来，我们在后面获得的损失函数对前面参数的求导，获得的梯度也会是相同的，根据学习率向着相同的方向做梯度更新以后，获得的参数也将会是相同的。最后在更新以后每层隐含层的参数是相同的，这就意味着整个神经网络参数高度冗余，在最后的输出单元只会得到单个功能。因此zero initialization 在神经网络中是不可行的</p><p>因此我们需要更有特色，更有用的参数权重初始化方法，使得我们的学习是有用的学习，在模型中能有更多的功能，或者这么说，我们可以获得一个更好的，较少冗余的神经网络模型。</p><h1 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h1><p>Random Initialization 是神经网络初始化的一种方法。</p><p>随机初始化的方法，给定初始化为 rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON</p><p>此处的INIT_EPSILON 是一个非常接近于0 的初始值,很好的一个选择方法是如下的公式</p><p>$$\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}} $$</p><p>此处的$L_{in} = s_l and L_{out} = s_{l+1}$ 为初始化参数$\theta$相应网络层中的神经元个数</p><p>通过这个方法初始化所有的参数，可以打破对称。从而跳出之前在使用Zero initialization 时候遇到的问题。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MachineLearning with SK AND TF (1_端到端的机器学习项目)</title>
      <link href="undefined2019/08/23/MachineLearning-with-SK-AND-TF-1-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
      <url>2019/08/23/MachineLearning-with-SK-AND-TF-1-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<pre><code class="python"># download datasetimport osimport tarfileimport urllibDOWNLOAD_ROOT = &quot;http://raw.githubusercontent.com/ageron/handson-ml/master/&quot;HOUSING_PATH = &quot;datasets/housing&quot;HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + &quot;/housing.tgz&quot;print(HOUSING_URL)def fetch_housing_data(housing_url = HOUSING_URL,housing_path = HOUSING_PATH):    if not os.path.isdir(housing_path):        os.makedirs(housing_path)    tgz_path = os.path.join(housing_path,&quot;housing.tgz&quot;)    urllib.request.urlretrieve(housing_url,tgz_path)    housing_tgz = tarfile.open(tgz_path)    housing_tgz.extractall(path=housing_path)    housing_tgz.close()</code></pre><pre><code>http://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz</code></pre><pre><code class="python"># load dataimport pandas as pddef load_housing_data(housing_path = HOUSING_PATH):    csv_path = os.path.join(housing_path, &quot;housing.csv&quot;)    return pd.read_csv(csv_path)</code></pre><pre><code class="python"># 调用if not os.path.isfile(HOUSING_PATH):    fetch_housing_data()housing = load_housing_data()housing</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>      <th>ocean_proximity</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-122.23</td>      <td>37.88</td>      <td>41.0</td>      <td>880.0</td>      <td>129.0</td>      <td>322.0</td>      <td>126.0</td>      <td>8.3252</td>      <td>452600.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>1</th>      <td>-122.22</td>      <td>37.86</td>      <td>21.0</td>      <td>7099.0</td>      <td>1106.0</td>      <td>2401.0</td>      <td>1138.0</td>      <td>8.3014</td>      <td>358500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>2</th>      <td>-122.24</td>      <td>37.85</td>      <td>52.0</td>      <td>1467.0</td>      <td>190.0</td>      <td>496.0</td>      <td>177.0</td>      <td>7.2574</td>      <td>352100.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>3</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1274.0</td>      <td>235.0</td>      <td>558.0</td>      <td>219.0</td>      <td>5.6431</td>      <td>341300.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>4</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1627.0</td>      <td>280.0</td>      <td>565.0</td>      <td>259.0</td>      <td>3.8462</td>      <td>342200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>5</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>919.0</td>      <td>213.0</td>      <td>413.0</td>      <td>193.0</td>      <td>4.0368</td>      <td>269700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>6</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>2535.0</td>      <td>489.0</td>      <td>1094.0</td>      <td>514.0</td>      <td>3.6591</td>      <td>299200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>7</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>3104.0</td>      <td>687.0</td>      <td>1157.0</td>      <td>647.0</td>      <td>3.1200</td>      <td>241400.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>8</th>      <td>-122.26</td>      <td>37.84</td>      <td>42.0</td>      <td>2555.0</td>      <td>665.0</td>      <td>1206.0</td>      <td>595.0</td>      <td>2.0804</td>      <td>226700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>9</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>3549.0</td>      <td>707.0</td>      <td>1551.0</td>      <td>714.0</td>      <td>3.6912</td>      <td>261100.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>10</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2202.0</td>      <td>434.0</td>      <td>910.0</td>      <td>402.0</td>      <td>3.2031</td>      <td>281500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>11</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>3503.0</td>      <td>752.0</td>      <td>1504.0</td>      <td>734.0</td>      <td>3.2705</td>      <td>241800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>12</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2491.0</td>      <td>474.0</td>      <td>1098.0</td>      <td>468.0</td>      <td>3.0750</td>      <td>213500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>13</th>      <td>-122.26</td>      <td>37.84</td>      <td>52.0</td>      <td>696.0</td>      <td>191.0</td>      <td>345.0</td>      <td>174.0</td>      <td>2.6736</td>      <td>191300.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>14</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2643.0</td>      <td>626.0</td>      <td>1212.0</td>      <td>620.0</td>      <td>1.9167</td>      <td>159200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>15</th>      <td>-122.26</td>      <td>37.85</td>      <td>50.0</td>      <td>1120.0</td>      <td>283.0</td>      <td>697.0</td>      <td>264.0</td>      <td>2.1250</td>      <td>140000.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>16</th>      <td>-122.27</td>      <td>37.85</td>      <td>52.0</td>      <td>1966.0</td>      <td>347.0</td>      <td>793.0</td>      <td>331.0</td>      <td>2.7750</td>      <td>152500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>17</th>      <td>-122.27</td>      <td>37.85</td>      <td>52.0</td>      <td>1228.0</td>      <td>293.0</td>      <td>648.0</td>      <td>303.0</td>      <td>2.1202</td>      <td>155500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>18</th>      <td>-122.26</td>      <td>37.84</td>      <td>50.0</td>      <td>2239.0</td>      <td>455.0</td>      <td>990.0</td>      <td>419.0</td>      <td>1.9911</td>      <td>158700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>19</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>1503.0</td>      <td>298.0</td>      <td>690.0</td>      <td>275.0</td>      <td>2.6033</td>      <td>162900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>20</th>      <td>-122.27</td>      <td>37.85</td>      <td>40.0</td>      <td>751.0</td>      <td>184.0</td>      <td>409.0</td>      <td>166.0</td>      <td>1.3578</td>      <td>147500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>21</th>      <td>-122.27</td>      <td>37.85</td>      <td>42.0</td>      <td>1639.0</td>      <td>367.0</td>      <td>929.0</td>      <td>366.0</td>      <td>1.7135</td>      <td>159800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>22</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>2436.0</td>      <td>541.0</td>      <td>1015.0</td>      <td>478.0</td>      <td>1.7250</td>      <td>113900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>23</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>1688.0</td>      <td>337.0</td>      <td>853.0</td>      <td>325.0</td>      <td>2.1806</td>      <td>99700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>24</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>2224.0</td>      <td>437.0</td>      <td>1006.0</td>      <td>422.0</td>      <td>2.6000</td>      <td>132600.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>25</th>      <td>-122.28</td>      <td>37.85</td>      <td>41.0</td>      <td>535.0</td>      <td>123.0</td>      <td>317.0</td>      <td>119.0</td>      <td>2.4038</td>      <td>107500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>26</th>      <td>-122.28</td>      <td>37.85</td>      <td>49.0</td>      <td>1130.0</td>      <td>244.0</td>      <td>607.0</td>      <td>239.0</td>      <td>2.4597</td>      <td>93800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>27</th>      <td>-122.28</td>      <td>37.85</td>      <td>52.0</td>      <td>1898.0</td>      <td>421.0</td>      <td>1102.0</td>      <td>397.0</td>      <td>1.8080</td>      <td>105500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>28</th>      <td>-122.28</td>      <td>37.84</td>      <td>50.0</td>      <td>2082.0</td>      <td>492.0</td>      <td>1131.0</td>      <td>473.0</td>      <td>1.6424</td>      <td>108900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>29</th>      <td>-122.28</td>      <td>37.84</td>      <td>52.0</td>      <td>729.0</td>      <td>160.0</td>      <td>395.0</td>      <td>155.0</td>      <td>1.6875</td>      <td>132000.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>20610</th>      <td>-121.56</td>      <td>39.10</td>      <td>28.0</td>      <td>2130.0</td>      <td>484.0</td>      <td>1195.0</td>      <td>439.0</td>      <td>1.3631</td>      <td>45500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20611</th>      <td>-121.55</td>      <td>39.10</td>      <td>27.0</td>      <td>1783.0</td>      <td>441.0</td>      <td>1163.0</td>      <td>409.0</td>      <td>1.2857</td>      <td>47000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20612</th>      <td>-121.56</td>      <td>39.08</td>      <td>26.0</td>      <td>1377.0</td>      <td>289.0</td>      <td>761.0</td>      <td>267.0</td>      <td>1.4934</td>      <td>48300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20613</th>      <td>-121.55</td>      <td>39.09</td>      <td>31.0</td>      <td>1728.0</td>      <td>365.0</td>      <td>1167.0</td>      <td>384.0</td>      <td>1.4958</td>      <td>53400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20614</th>      <td>-121.54</td>      <td>39.08</td>      <td>26.0</td>      <td>2276.0</td>      <td>460.0</td>      <td>1455.0</td>      <td>474.0</td>      <td>2.4695</td>      <td>58000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20615</th>      <td>-121.54</td>      <td>39.08</td>      <td>23.0</td>      <td>1076.0</td>      <td>216.0</td>      <td>724.0</td>      <td>197.0</td>      <td>2.3598</td>      <td>57500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20616</th>      <td>-121.53</td>      <td>39.08</td>      <td>15.0</td>      <td>1810.0</td>      <td>441.0</td>      <td>1157.0</td>      <td>375.0</td>      <td>2.0469</td>      <td>55100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20617</th>      <td>-121.53</td>      <td>39.06</td>      <td>20.0</td>      <td>561.0</td>      <td>109.0</td>      <td>308.0</td>      <td>114.0</td>      <td>3.3021</td>      <td>70800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20618</th>      <td>-121.55</td>      <td>39.06</td>      <td>25.0</td>      <td>1332.0</td>      <td>247.0</td>      <td>726.0</td>      <td>226.0</td>      <td>2.2500</td>      <td>63400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20619</th>      <td>-121.56</td>      <td>39.01</td>      <td>22.0</td>      <td>1891.0</td>      <td>340.0</td>      <td>1023.0</td>      <td>296.0</td>      <td>2.7303</td>      <td>99100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20620</th>      <td>-121.48</td>      <td>39.05</td>      <td>40.0</td>      <td>198.0</td>      <td>41.0</td>      <td>151.0</td>      <td>48.0</td>      <td>4.5625</td>      <td>100000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20621</th>      <td>-121.47</td>      <td>39.01</td>      <td>37.0</td>      <td>1244.0</td>      <td>247.0</td>      <td>484.0</td>      <td>157.0</td>      <td>2.3661</td>      <td>77500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20622</th>      <td>-121.44</td>      <td>39.00</td>      <td>20.0</td>      <td>755.0</td>      <td>147.0</td>      <td>457.0</td>      <td>157.0</td>      <td>2.4167</td>      <td>67000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20623</th>      <td>-121.37</td>      <td>39.03</td>      <td>32.0</td>      <td>1158.0</td>      <td>244.0</td>      <td>598.0</td>      <td>227.0</td>      <td>2.8235</td>      <td>65500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20624</th>      <td>-121.41</td>      <td>39.04</td>      <td>16.0</td>      <td>1698.0</td>      <td>300.0</td>      <td>731.0</td>      <td>291.0</td>      <td>3.0739</td>      <td>87200.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20625</th>      <td>-121.52</td>      <td>39.12</td>      <td>37.0</td>      <td>102.0</td>      <td>17.0</td>      <td>29.0</td>      <td>14.0</td>      <td>4.1250</td>      <td>72000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20626</th>      <td>-121.43</td>      <td>39.18</td>      <td>36.0</td>      <td>1124.0</td>      <td>184.0</td>      <td>504.0</td>      <td>171.0</td>      <td>2.1667</td>      <td>93800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20627</th>      <td>-121.32</td>      <td>39.13</td>      <td>5.0</td>      <td>358.0</td>      <td>65.0</td>      <td>169.0</td>      <td>59.0</td>      <td>3.0000</td>      <td>162500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20628</th>      <td>-121.48</td>      <td>39.10</td>      <td>19.0</td>      <td>2043.0</td>      <td>421.0</td>      <td>1018.0</td>      <td>390.0</td>      <td>2.5952</td>      <td>92400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20629</th>      <td>-121.39</td>      <td>39.12</td>      <td>28.0</td>      <td>10035.0</td>      <td>1856.0</td>      <td>6912.0</td>      <td>1818.0</td>      <td>2.0943</td>      <td>108300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20630</th>      <td>-121.32</td>      <td>39.29</td>      <td>11.0</td>      <td>2640.0</td>      <td>505.0</td>      <td>1257.0</td>      <td>445.0</td>      <td>3.5673</td>      <td>112000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20631</th>      <td>-121.40</td>      <td>39.33</td>      <td>15.0</td>      <td>2655.0</td>      <td>493.0</td>      <td>1200.0</td>      <td>432.0</td>      <td>3.5179</td>      <td>107200.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20632</th>      <td>-121.45</td>      <td>39.26</td>      <td>15.0</td>      <td>2319.0</td>      <td>416.0</td>      <td>1047.0</td>      <td>385.0</td>      <td>3.1250</td>      <td>115600.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20633</th>      <td>-121.53</td>      <td>39.19</td>      <td>27.0</td>      <td>2080.0</td>      <td>412.0</td>      <td>1082.0</td>      <td>382.0</td>      <td>2.5495</td>      <td>98300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20634</th>      <td>-121.56</td>      <td>39.27</td>      <td>28.0</td>      <td>2332.0</td>      <td>395.0</td>      <td>1041.0</td>      <td>344.0</td>      <td>3.7125</td>      <td>116800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20635</th>      <td>-121.09</td>      <td>39.48</td>      <td>25.0</td>      <td>1665.0</td>      <td>374.0</td>      <td>845.0</td>      <td>330.0</td>      <td>1.5603</td>      <td>78100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20636</th>      <td>-121.21</td>      <td>39.49</td>      <td>18.0</td>      <td>697.0</td>      <td>150.0</td>      <td>356.0</td>      <td>114.0</td>      <td>2.5568</td>      <td>77100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20637</th>      <td>-121.22</td>      <td>39.43</td>      <td>17.0</td>      <td>2254.0</td>      <td>485.0</td>      <td>1007.0</td>      <td>433.0</td>      <td>1.7000</td>      <td>92300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20638</th>      <td>-121.32</td>      <td>39.43</td>      <td>18.0</td>      <td>1860.0</td>      <td>409.0</td>      <td>741.0</td>      <td>349.0</td>      <td>1.8672</td>      <td>84700.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20639</th>      <td>-121.24</td>      <td>39.37</td>      <td>16.0</td>      <td>2785.0</td>      <td>616.0</td>      <td>1387.0</td>      <td>530.0</td>      <td>2.3886</td>      <td>89400.0</td>      <td>INLAND</td>    </tr>  </tbody></table><p>20640 rows × 10 columns</p></div><pre><code class="python">housing.info()</code></pre><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 20640 entries, 0 to 20639Data columns (total 10 columns):longitude             20640 non-null float64latitude              20640 non-null float64housing_median_age    20640 non-null float64total_rooms           20640 non-null float64total_bedrooms        20433 non-null float64population            20640 non-null float64households            20640 non-null float64median_income         20640 non-null float64median_house_value    20640 non-null float64ocean_proximity       20640 non-null objectdtypes: float64(9), object(1)memory usage: 1.6+ MB</code></pre><pre><code class="python"># 查看数据集housing.describe()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20433.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>    </tr>    <tr>      <th>mean</th>      <td>-119.569704</td>      <td>35.631861</td>      <td>28.639486</td>      <td>2635.763081</td>      <td>537.870553</td>      <td>1425.476744</td>      <td>499.539680</td>      <td>3.870671</td>      <td>206855.816909</td>    </tr>    <tr>      <th>std</th>      <td>2.003532</td>      <td>2.135952</td>      <td>12.585558</td>      <td>2181.615252</td>      <td>421.385070</td>      <td>1132.462122</td>      <td>382.329753</td>      <td>1.899822</td>      <td>115395.615874</td>    </tr>    <tr>      <th>min</th>      <td>-124.350000</td>      <td>32.540000</td>      <td>1.000000</td>      <td>2.000000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>1.000000</td>      <td>0.499900</td>      <td>14999.000000</td>    </tr>    <tr>      <th>25%</th>      <td>-121.800000</td>      <td>33.930000</td>      <td>18.000000</td>      <td>1447.750000</td>      <td>296.000000</td>      <td>787.000000</td>      <td>280.000000</td>      <td>2.563400</td>      <td>119600.000000</td>    </tr>    <tr>      <th>50%</th>      <td>-118.490000</td>      <td>34.260000</td>      <td>29.000000</td>      <td>2127.000000</td>      <td>435.000000</td>      <td>1166.000000</td>      <td>409.000000</td>      <td>3.534800</td>      <td>179700.000000</td>    </tr>    <tr>      <th>75%</th>      <td>-118.010000</td>      <td>37.710000</td>      <td>37.000000</td>      <td>3148.000000</td>      <td>647.000000</td>      <td>1725.000000</td>      <td>605.000000</td>      <td>4.743250</td>      <td>264725.000000</td>    </tr>    <tr>      <th>max</th>      <td>-114.310000</td>      <td>41.950000</td>      <td>52.000000</td>      <td>39320.000000</td>      <td>6445.000000</td>      <td>35682.000000</td>      <td>6082.000000</td>      <td>15.000100</td>      <td>500001.000000</td>    </tr>  </tbody></table></div><pre><code class="python">%matplotlib inlineimport matplotlib.pyplot as plthousing.hist(bins=50, figsize=(20,15))plt.show()</code></pre><p><img src="output_5_0.png" alt="png"></p><pre><code class="python"># Prpare data## Train Test splitfrom sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(housing,test_size=0.2 , random_state = 42)</code></pre><pre><code class="python"># 上面的方法的缺陷，对于某个数据集，我们不能保证训练集和测试集都有这个字段的每个类别，因此我们需要按照下列方法使用分层抽样的方法对训练测试数据集进行分割割## 分层抽样 对income_cat进行抽样import numpy as np### 根据收入分层，变成几个类别，数据集乘1.5限制收入类别数量，使用ceil对数据集进行取整，得到离散类别housing[&quot;income_cat&quot;] = np.ceil(housing[&quot;median_income&quot;] / 1.5)housing[&quot;income_cat&quot;].where(housing[&quot;income_cat&quot;] &lt;5 ,5.0, inplace = True) # where 替换大于等于5.0的数据为5.0# 对incomg收入进行分类以后，进行分层抽样from sklearn.model_selection import StratifiedShuffleSplit # 分层洗牌拆分split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state = 42)for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): # 在数据集中使用income_cat 对housing进行分层抽样    strat_train_set = housing.loc[train_index]    strat_test_set = housing.loc[test_index]# 查看数据集中每个分层所占比例housing[&quot;income_cat&quot;].value_counts() / len(housing)</code></pre><pre><code>3.0    0.3505812.0    0.3188474.0    0.1763085.0    0.1144381.0    0.039826Name: income_cat, dtype: float64</code></pre><pre><code class="python"># drop income_catfor setdt in (strat_train_set,strat_test_set):    setdt.drop([&#39;income_cat&#39;],axis = 1 , inplace = True)</code></pre><pre><code class="python">housing = strat_train_set.copy()housing.plot(kind = &quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;,alpha = 0.4,            s = housing[&quot;population&quot;]/100,label = &quot;population&quot;,            c = &quot;median_house_value&quot;,cmap = plt.get_cmap(&quot;jet&quot;),colorbar = True)plt.legend()</code></pre><pre><code>&lt;matplotlib.legend.Legend at 0x142354e0&gt;</code></pre><p><img src="output_9_1.png" alt="png"></p><pre><code class="python"># 数据集不大，可以使用corr()函数计算没对属性之间“标准相关系数（皮尔逊相关系数）”corr_metrix = housing.corr()corr_metrix[&quot;median_house_value&quot;].sort_values(ascending = False)</code></pre><pre><code>median_house_value    1.000000median_income         0.687160total_rooms           0.135097housing_median_age    0.114110households            0.064506total_bedrooms        0.047689population           -0.026920longitude            -0.047432latitude             -0.142724Name: median_house_value, dtype: float64</code></pre><pre><code class="python">from pandas.plotting import scatter_matrix# scatter_matrix 绘制出每个数值属性相对于其他数值属性的相关性，在此我们有11个数值属性，可以得到11**2 = 121图像，我们可以选取相应的数值展示attributes = [&quot;median_house_value&quot;,&quot;median_income&quot;,&quot;total_rooms&quot;,&quot;housing_median_age&quot;]scatter_matrix(housing[attributes],figsize = (12,8))# 在对角线上原本为自身对照，这没有意义，因此，pandas取而代之的是每个属性的直方图</code></pre><pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001495A630&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001470D048&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000147326D8&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014758D68&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001478B438&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001478B470&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014AB5160&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014ADE7F0&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B07E80&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B38550&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B5FBE0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B922B0&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014DAA940&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014DD1FD0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014E036A0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001504CD30&gt;]],      dtype=object)</code></pre><p><img src="output_11_1.png" alt="png"></p><pre><code class="python"># 尝试根据现有数据集创建新的觉得有用的数据housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;] / housing[&quot;households&quot;]housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;] / housing[&quot;total_rooms&quot;]housing[&quot;population_per_household&quot;] = housing[&quot;population&quot;] / housing[&quot;households&quot;]</code></pre><hr><pre><code class="python"># 重新获取一组新数据我们用来处理housing = strat_train_set.drop(&quot;median_house_value&quot;,axis = 1)housing_labels = strat_train_set[&quot;median_house_value&quot;].copy()housing.info()housing.shape</code></pre><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;Int64Index: 16512 entries, 17606 to 15775Data columns (total 9 columns):longitude             16512 non-null float64latitude              16512 non-null float64housing_median_age    16512 non-null float64total_rooms           16512 non-null float64total_bedrooms        16354 non-null float64population            16512 non-null float64households            16512 non-null float64median_income         16512 non-null float64ocean_proximity       16512 non-null objectdtypes: float64(8), object(1)memory usage: 1.3+ MB(16512, 9)</code></pre><pre><code class="python"># 处理数值属性## 对于属性缺失的问题，如上面的total_badrooms属性:# housing.dropna(subset=[&quot;total_bedrooms&quot;])### 放弃相应地区# housing.drop(&quot;total_bedrooms&quot;,axis =1)### 放弃这个属性# median = housing[&quot;total_bedrooms&quot;].median()# housing[&quot;total_bedrooms&quot;].fillna(median)### 将缺失属性设置为某个默认值（0，平均数，中位数）from sklearn.preprocessing import Imputer# 不确定其他属性值是否会存在缺失值，就把inputer应用于所有属性imputer = Imputer(strategy = &quot;median&quot;)### 使用sklearn中的api可以轻松实现第三种housing_num = housing.drop(&quot;ocean_proximity&quot;,axis = 1) # 创建临时数据集，该数据集没有文本属性imputer.fit(housing_num) # inputer将处理后的中位数存放于statistics_中X = imputer.transform(housing_num) # 回传numpy数组housing_tr = pd.DataFrame(X,columns=housing_num.columns) # 转为pandas</code></pre><pre><code class="python"># 处理文本和分类属性housing_cat = housing[&quot;ocean_proximity&quot;]&quot;&quot;&quot;## sklearn提供LabelEncoderfrom sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()housing_cat_encoded = encoder.fit_transform(housing_cat)housing_cat_encoded## 将label转为onehot编码from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))housing_cat_1hot.toarray()&quot;&quot;&quot;# LabelBinarizer from sklearn.preprocessing import LabelBinarizerencoder = LabelBinarizer()housing_cat_1hot = encoder.fit_transform(housing_cat)# 我们可以自定义转换器from sklearn.base import BaseEstimator,TransformerMixinrooms_ix, bedrooms_ix,population_ix, household_ix = 3,4,5,6class CombinedAttributesAdder(BaseEstimator,TransformerMixin):    def __init__(self, add_bedrooms_per_rooms = True):        self.add_bedrooms_per_rooms = add_bedrooms_per_rooms    def fit(self,X,y = None):        return self    def transform(self,X,y = None):        rooms_per_household = X[:,rooms_ix] / X[:,household_ix]        population_per_household = X[:,population_ix] / X[:,household_ix]        if self.add_bedrooms_per_rooms:            bedrooms_per_rooms = X[:,bedrooms_ix] / X[:,rooms_ix]            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_rooms]        else:            return np.c_[X,rooms_per_household,population_per_household]sttr_adder = CombinedAttributesAdder(add_bedrooms_per_rooms = False)housing_extra_attribs = sttr_adder.transform(housing.values)</code></pre><pre><code class="python">## 使用pipeline创建流水线转换处理数据from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScaler # 数据标准化num_pipeline = Pipeline([    (&#39;imputer&#39;, Imputer(strategy=&#39;median&#39;)),    (&#39;attribs_adder&#39;, CombinedAttributesAdder()),    (&#39;std_scaler&#39;,StandardScaler())])# 每个转换器都需要有fit_transform方法，调用fit函数则会按照顺序执行fit_transform(),如果不希望调用完fit()后再调用transform，可以直接调用fit_transformhousing_num_tr = num_pipeline.fit_transform(housing_num)class DataFrameSelector(BaseEstimator,TransformerMixin):    def __init__(self,attribute_names):        self.attribute_names = attribute_names    def fit(self,X,y = None):        return self    def transform(self,X,y = None):        return X[self.attribute_names].values# 由于LabelBinarizer默认传参为2，后面调用会报错，可以定义一个自己的LabelBinarizer类class MyLabelBinarizer(TransformerMixin):    def __init__(self,*args, **kwargs):        self.encoder = LabelBinarizer(*args, **kwargs)    def fit(self,X,y = None):        self.encoder.fit(X)        return self    def transform(self,X,y = None):        return self.encoder.transform(X)# 如果有多个pipeline，我们可以使用如下方法进行拼接from sklearn.pipeline import FeatureUnionnum_attribs = list(housing_num)cat_attribs = [&#39;ocean_proximity&#39;]num_pipeline = Pipeline([    (&#39;selector&#39;, DataFrameSelector(num_attribs)),    (&#39;imputer&#39;, Imputer(strategy=&#39;median&#39;)),    (&#39;attribs_adder&#39;, CombinedAttributesAdder()),    (&#39;std_scaler&#39;,StandardScaler())])cat_pipeline = Pipeline([    (&#39;selector&#39;,DataFrameSelector(cat_attribs)),    (&#39;label_binarizer&#39;, MyLabelBinarizer()),])full_pipeline = FeatureUnion(transformer_list = [    (&quot;num_pipeline&quot;,num_pipeline),    (&quot;cat_pipeline&quot;,cat_pipeline),])housing_prepared = full_pipeline.fit_transform(housing)print(housing_prepared)print(housing_prepared.shape)print(housing_prepared[0])</code></pre><pre><code>[[-1.15604281  0.77194962  0.74333089 ...  0.          0.   0.        ] [-1.17602483  0.6596948  -1.1653172  ...  0.          0.   0.        ] [ 1.18684903 -1.34218285  0.18664186 ...  0.          0.   1.        ] ... [ 1.58648943 -0.72478134 -1.56295222 ...  0.          0.   0.        ] [ 0.78221312 -0.85106801  0.18664186 ...  0.          0.   0.        ] [-1.43579109  0.99645926  1.85670895 ...  0.          1.   0.        ]](16512, 16)[-1.15604281  0.77194962  0.74333089 -0.49323393 -0.44543821 -0.63621141 -0.42069842 -0.61493744 -0.31205452 -0.08649871  0.15531753  1.  0.          0.          0.          0.        ]</code></pre><hr><h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><pre><code class="python"># 线性回归模型的训练from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</code></pre><pre><code class="python"># 使用几个训练集的实例测试我们的线性回归模型some_data = housing.iloc[:5]some_datasome_labels = housing_labels.iloc[:5]some_data_prepared = full_pipeline.transform(some_data)print(&quot;Predictions: \t&quot;, lin_reg.predict(some_data_prepared))print(&quot;Labels:\t\t&quot;,list(some_labels)) # 虽然可以工作了预测准确度太差# 可以使用sklearn中RMSE(mean-squared-error)测量训练集上回归模型的RMSEfrom sklearn.metrics import mean_squared_errorhousing_predictions = lin_reg.predict(housing_prepared)lin_mse = mean_squared_error(housing_labels,housing_predictions)print(&quot;Train-MSE:%s&quot;%lin_mse)lin_rmse = np.sqrt(lin_mse)print(&quot;Train-RMSE：%s&quot;%lin_rmse)</code></pre><pre><code>Predictions:      [210644.60459286 317768.80697211 210956.43331178  59218.98886849 189747.55849879]Labels:         [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]Train-MSE:4709829587.971121Train-RMSE：68628.19819848923</code></pre><pre><code class="python"># 使用sklearn中的交叉验证功能# 降训练集随机分成10份，每个子集称为一个折叠fold，对决策树模型进行十次训练和评估，每次调训一个折叠作为评估，另外9份作为训练集。from sklearn.model_selection import cross_val_scorelin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=&quot;neg_mean_squared_error&quot;,cv = 10)lin_rmse_scores = np.sqrt(-lin_scores)print(&#39;Scores:&#39;,lin_rmse_scores)print(&#39;Mean（均值）:&#39;,lin_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,lin_rmse_scores.std())</code></pre><pre><code>Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067]Mean（均值）: 69052.46136345083Standard deviation(标准偏差): 2731.6740017983425</code></pre><hr><h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><pre><code class="python">from sklearn.tree import DecisionTreeClassifiertree_reg = DecisionTreeClassifier()tree_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,            max_features=None, max_leaf_nodes=None,            min_impurity_decrease=0.0, min_impurity_split=None,            min_samples_leaf=1, min_samples_split=2,            min_weight_fraction_leaf=0.0, presort=False, random_state=None,            splitter=&#39;best&#39;)</code></pre><pre><code class="python">housing_predictions = tree_reg.predict(housing_prepared)tree_mse = mean_squared_error(housing_labels,housing_predictions)tree_rmse = np.sqrt(tree_mse)tree_rmse #已过拟合，我们可以使用之前分割的测试数据集进行验证或者使用交叉验证</code></pre><pre><code>0.0</code></pre><pre><code class="python">tree_scores = cross_val_score(tree_reg,housing_prepared,housing_labels,scoring = &#39;neg_mean_squared_error&#39;,cv = 10)tree_rmse_scores = np.sqrt(-tree_scores)</code></pre><pre><code>d:\ipython\lib\site-packages\sklearn\model_selection\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.  % (min_groups, self.n_splits)), Warning)</code></pre><pre><code class="python">print(&#39;Scores:&#39;,tree_rmse_scores)print(&#39;Mean（均值）:&#39;,tree_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,tree_rmse_scores.std())</code></pre><pre><code>Scores: [ 85710.01369961  81230.61378336  82335.83537005  75877.27486535  75090.99888798  78409.26002322  78475.89222942  79428.78633416 103606.00563702 101828.54775259]Mean（均值）: 84199.322858276Standard deviation(标准偏差): 9712.031022021029</code></pre><hr><h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><pre><code class="python">from sklearn.ensemble import RandomForestRegressorforest_reg = RandomForestRegressor()forest_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=&#39;auto&#39;, max_leaf_nodes=None,           min_impurity_decrease=0.0, min_impurity_split=None,           min_samples_leaf=1, min_samples_split=2,           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,           oob_score=False, random_state=None, verbose=0, warm_start=False)</code></pre><pre><code class="python">housing_predictions = forest_reg.predict(housing_prepared)forest_mse = mean_squared_error(housing_labels,housing_predictions)forest_rmse = np.sqrt(forest_mse)forest_rmse</code></pre><pre><code>22174.812789705677</code></pre><pre><code class="python">forest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring = &#39;neg_mean_squared_error&#39;,cv = 10)forest_rmse_scores = np.sqrt(-forest_scores)print(&#39;Scores:&#39;,forest_rmse_scores)print(&#39;Mean（均值）:&#39;,forest_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,forest_rmse_scores.std())</code></pre><pre><code>Scores: [52673.69656378 50803.5706211  53741.47818017 53343.47330547 51028.26110671 56342.5218843  50757.88436481 50711.95588612 55791.4729642  52200.01381263]Mean（均值）: 52739.43286892828Standard deviation(标准偏差): 1966.5788024649446</code></pre><hr><h1 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h1><p>上面的模型我们知识大概了解一下。现在我们有几组比较有效的模型候选列表，我们可以使用网格搜索的方式去调整超参数</p><p>下面我们使用GridSearchCV对RandomForestRegressor进行网格搜索调整参数</p><pre><code class="python">from sklearn.model_selection import GridSearchCVparam_guid = [    {&#39;n_estimators&#39;:[3,10,30],&#39;max_features&#39;:[2,4,6,8]},    {&#39;bootstrap&#39;:[False],&#39;n_estimators&#39;:[3,10],&#39;max_features&#39;:[2,3,4]},]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg,param_guid,cv = 5,scoring = &#39;neg_mean_squared_error&#39;)grid_search.fit(housing_prepared,housing_labels)</code></pre><pre><code>GridSearchCV(cv=5, error_score=&#39;raise&#39;,       estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=&#39;auto&#39;, max_leaf_nodes=None,           min_impurity_decrease=0.0, min_impurity_split=None,           min_samples_leaf=1, min_samples_split=2,           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,           oob_score=False, random_state=None, verbose=0, warm_start=False),       fit_params=None, iid=True, n_jobs=1,       param_grid=[{&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}],       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,       scoring=&#39;neg_mean_squared_error&#39;, verbose=0)</code></pre><pre><code class="python"># 搜索完成以后你将会得到在你给予的值范围内最好的超参数，# 因为n_estimators超参数在我们的搜索中30是最大值，我们获取可以继续增大该值，或许能获取到更优的超参数print(grid_search.best_params_) # 最好的超参数print(grid_search.best_estimator_) # 获取最好的模型cvres = grid_search.cv_results_ # 所有策略评估分数for mean_score, params in zip(cvres[&quot;mean_test_score&quot;],cvres[&quot;params&quot;]):    print(np.sqrt(-mean_score),params)</code></pre><pre><code>{&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30}RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,           min_impurity_split=None, min_samples_leaf=1,           min_samples_split=2, min_weight_fraction_leaf=0.0,           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,           verbose=0, warm_start=False)64119.85349022265 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}55381.23545742357 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}53075.44430708791 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30}60409.23914852959 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}52752.70703014053 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}50688.20150002824 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30}58807.05957827667 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3}52001.301422501005 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10}50192.68760353124 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30}58407.14908967587 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3}52082.05779305404 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10}50055.361023543235 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30}63019.801729902276 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}54226.00106549722 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}61102.60574526821 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3}52792.63987300767 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10}59220.41356163033 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}51283.813598815985 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}</code></pre><h2 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h2><p>随机搜索与网格搜索使用方法类似，sklearn中的方法 RandomizedSearchCV</p><h2 id="集成"><a href="#集成" class="headerlink" title="集成"></a>集成</h2><p>就是把比较好的几种方法进行集成，类似于随机森林</p><hr><h1 id="分析最佳模型及其错误"><a href="#分析最佳模型及其错误" class="headerlink" title="分析最佳模型及其错误"></a>分析最佳模型及其错误</h1><pre><code class="python"># 我们在进行准确预估的时候，可以指出每个属性相对的重要程度feature_importances = grid_search.best_estimator_.feature_importances_print(feature_importances)extra_attribs = [&#39;rooms_per_household&#39;,&#39;population_per_household&#39;,&#39;bedrooms_per_room&#39;]cat_one_hot_attribs = list(encoder.classes_)attributes = num_attribs + extra_attribs + cat_one_hot_attribssorted(zip(feature_importances, attributes),reverse= True)</code></pre><pre><code>[7.11837050e-02 6.49390013e-02 4.45982246e-02 1.54058658e-02 1.51343704e-02 1.51144514e-02 1.43021595e-02 3.32747738e-01 5.47807317e-02 1.11495774e-01 8.53794690e-02 4.73000789e-03 1.64498398e-01 7.99754238e-05 2.58803593e-03 3.02209250e-03][(0.3327477377793599, &#39;median_income&#39;), (0.16449839819355594, &#39;INLAND&#39;), (0.11149577356183278, &#39;population_per_household&#39;), (0.08537946903014358, &#39;bedrooms_per_room&#39;), (0.07118370498819128, &#39;longitude&#39;), (0.06493900130495682, &#39;latitude&#39;), (0.05478073171138477, &#39;rooms_per_household&#39;), (0.04459822463776366, &#39;housing_median_age&#39;), (0.015405865798022386, &#39;total_rooms&#39;), (0.015134370410352954, &#39;total_bedrooms&#39;), (0.015114451369986391, &#39;population&#39;), (0.014302159478332245, &#39;households&#39;), (0.0047300078856028485, &#39;&lt;1H OCEAN&#39;), (0.0030220924972792274, &#39;NEAR OCEAN&#39;), (0.0025880359294569487, &#39;NEAR BAY&#39;), (7.997542377816801e-05, &#39;ISLAND&#39;)]</code></pre><hr><h1 id="通过测试集评估系统"><a href="#通过测试集评估系统" class="headerlink" title="通过测试集评估系统"></a>通过测试集评估系统</h1><pre><code class="python">final_model = grid_search.best_estimator_X_test = strat_test_set.drop(&quot;median_house_value&quot;,axis = 1)y_test = strat_test_set[&quot;median_house_value&quot;].copy()x_test_prepared = full_pipeline.transform(X_test)final_predictions = final_model.predict(x_test_prepared)final_mse = mean_squared_error(y_test,final_predictions)final_rmse = np.sqrt(final_mse)final_rmse</code></pre><pre><code>47913.84534663264</code></pre><p>模型已经成型，进入项目预启动阶段，你需要展示你的解决方案（强调学习了什么，有什么用，什么没有用，基于什么假设，以及系统的限制有什么），记录所有事情，通过清晰的可视化和易于记忆的陈述方式，制作漂亮的演示文稿</p><hr><h1 id="启动、监控、维护系统"><a href="#启动、监控、维护系统" class="headerlink" title="启动、监控、维护系统"></a>启动、监控、维护系统</h1><ul><li>做好生产数据接入系统准备</li><li>编写监控代码定期检查实时性能，同时在性能下降时发出警告</li><li>还需要评估输入系统的数据质量</li><li>尽可能自动化完成一定的时间自训练模型，如果是在线的学习系统，应该定期保存系统状态，快速备份</li></ul><h1 id="START"><a href="#START" class="headerlink" title="START"></a>START</h1><p><a href="http://kaggle.com/" target="_blank" rel="noopener">http://kaggle.com/</a> 是一个不错的网站，它会给你一个数据集，一个明确的目标，同时也可一起分享经验</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> scikitlearn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> BOOK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oracle 中 varchar2 &amp; varchar </title>
      <link href="undefined2019/08/23/Oracle-%E4%B8%AD-varchar2-varchar/"/>
      <url>2019/08/23/Oracle-%E4%B8%AD-varchar2-varchar/</url>
      
        <content type="html"><![CDATA[<h1 id="直观"><a href="#直观" class="headerlink" title="直观"></a>直观</h1><p>varchar – 存放定長的字符数据，最長2000个字符；</p><p>varchar2 – 存放可变长字符数据，最大长度为4000字符。</p><h1 id="隐含"><a href="#隐含" class="headerlink" title="隐含"></a>隐含</h1><p>1.varchar2把所有字符都占两字节处理(一般情况下)，varchar只对汉字和全角等字符占两字节，数字，英文字符等都是一个字节</p><p>2.VARCHAR2把空串等同于null处理，而varchar仍按照空串处理</p><p>3.VARCHAR2字符要用几个字节存储，要看数据库使用的字符集</p><p><font color="red">注意：varchar 在获取到字符串时，若字符长度不够，会在数据后方以空格补齐保证定长， varchar2 会以原数据内容进行存储</font></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> Oracle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Pull 与 Git Fetch 的区别</title>
      <link href="undefined2019/08/23/Git-Pull-%E4%B8%8E-Git-Fetch-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>2019/08/23/Git-Pull-%E4%B8%8E-Git-Fetch-%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>说来也惭愧,这是在一场面试中,面试官口述问我的一道题。面试官的原话是这样的：你平时常用Git的话,那请问<code>Git Pull</code>和<code>Git Fetch</code>有什么区别？</p><p>当时我就蒙住了，Git在平时的使用中是挺常用的，公司平时都是小项目，对于代码安全来说并不是特别在意，平时也就用<code>Git Pull</code>直接把服务器上最新的代码给拉取到主分支，再创建分支，在分支上做开发,最后再合并。</p><p>平时也没查过fetch相关的内容，这可不，就只能栽在这了。</p><h1 id="Git-Pull"><a href="#Git-Pull" class="headerlink" title="Git Pull"></a>Git Pull</h1><p><code>Git Pull</code> 命令是Git当中将远程版本库中最新的内容拉取到本地的操作，该操作会直接将远程版本库中的内容直接拉取到本地，并将本地内容与拉取的内容做合并，在不经过任何统一的情况下，只要不遇见版本冲突等其他问题，就相当于直接替换了本地的版本内容。</p><p>用法：<code>git pull origin master</code></p><h1 id="Git-fetch"><a href="#Git-fetch" class="headerlink" title="Git fetch"></a>Git fetch</h1><p><code>Git Fetch</code> 命令是将远程版本库中的最新版本内容单纯拉取到本地的操作，拉取到本地的内容将会成为一个单独的分支，如果不进行后续的操作，主分支将不会做任何修改，也不会替换为远程版本库中的最新内容。需要通过查看是否需要当前更新的内容，然后再将此分支与本地主分支合并。</p><p>用法：<code>git fetch origin master</code>,<code>git log -p master.. origin/master</code>查看更改后，判断是否需要此项更新内容，需要则将分支进行合并 <code>git merge origin/master</code></p><h1 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h1><p>两者的功能大致相同，不同点就在于<code>git pull</code>是将版本直接拉取并合并到本地分支，而<code>git fetch</code>的操作则可以控制中间的步骤，查看更新内容，判断是否需要更新，相对于<code>git pull</code>来说，更加安全。</p><p>用何种方式对于大家来说应该是自己心里有底。<code>git pull</code> 方便，在小规模，互相信任的时候，比较适用，但在多人协作的情况下并不安全，<code>git fetch</code>对于大规模，多人协作的情况下有很大的优势。</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（三）</title>
      <link href="undefined2018/10/22/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>2018/10/22/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="管理者应该具备的管理技能"><a href="#管理者应该具备的管理技能" class="headerlink" title="管理者应该具备的管理技能"></a>管理者应该具备的管理技能</h1><p>沟通协调可行性方案</p><ol><li>执行控制</li><li>目标计划</li><li>指挥授权</li><li>培养激励</li></ol><h1 id="沟通的六个步骤"><a href="#沟通的六个步骤" class="headerlink" title="沟通的六个步骤"></a>沟通的六个步骤</h1><ol><li>事前准备</li><li>融洽关系</li><li>调查需求</li><li>阐述观点</li><li>处理异议</li><li>达成共识</li></ol><h1 id="表扬的艺术"><a href="#表扬的艺术" class="headerlink" title="表扬的艺术"></a>表扬的艺术</h1><p>高层次的表扬 ， 低层次的批评</p><p>表扬可以对人   批评需要对事</p><h1 id="沟通"><a href="#沟通" class="headerlink" title="沟通"></a>沟通</h1><p>沟通应该营造良好的氛围。当别人有想法时，应该选择倾听，适当记好笔记。对话的衔接应该多肯定，表达观点应该逻辑清晰，及时响应。沟通思想的同事也需要沟通情感</p><h1 id="沟通的方式"><a href="#沟通的方式" class="headerlink" title="沟通的方式"></a>沟通的方式</h1><p>涉及到敏感的问题，需要当面说，遇到决策性问题，应该以会议的形式，后期并以纸面形式确定会议内容。</p><p>有的沟通需要冷媒体（白纸黑字），实现有据可查</p><p>有的沟通需要热媒体（面对面的沟通），可以深入人心</p><p>有的沟通需要多媒体，重要性、准确性、多对象性</p><p><code>注意兼顾效率和效果，查找最可靠的沟通方式</code></p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（二）</title>
      <link href="undefined2018/10/16/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>2018/10/16/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="1-别人心目中理想的管理者"><a href="#1-别人心目中理想的管理者" class="headerlink" title="1.别人心目中理想的管理者"></a>1.别人心目中理想的管理者</h1><p>换位思考：发现领导眼中的我什么样的，发现下属眼中我是什么样的</p><p>从而领悟自己在领导眼中和下属眼中最应该提升什么</p><h1 id="2-层级意识"><a href="#2-层级意识" class="headerlink" title="2.层级意识"></a>2.层级意识</h1><p>建立信任、坦诚关系</p><p>站在上司角度看问题</p><p>试探上司让上司授权，提出合理化的建议，不越级指挥和汇报</p><p>及时提醒上司武略的事情</p><p>不盲从，合理“管理”上司</p><h1 id="3-管理者意识"><a href="#3-管理者意识" class="headerlink" title="3.管理者意识"></a>3.管理者意识</h1><p>①做管理者要先有 目标感 、全局感</p><p>②不漠视规则</p><p>③做事，快准狠</p><p>④不要低头做事，要学会抬头看路</p><p>⑤循规蹈矩不一定永远有效</p><p>⑥适当敏感和质疑现在的路线是必要的</p><p>⑦及时纠偏非常重要</p><h1 id="4-规矩"><a href="#4-规矩" class="headerlink" title="4.规矩"></a>4.规矩</h1><p>所有规矩，先坚决执行后，再提出相应的建议</p><h1 id="5-心态正面积极"><a href="#5-心态正面积极" class="headerlink" title="5.心态正面积极"></a>5.心态正面积极</h1><p>做企业中的发光体，不做黑洞，情绪A（成因）B（看法）C（后果）疗法，把握可控的，控制个人对时间的看法，改变事件的结果</p><h1 id="6-合作共赢"><a href="#6-合作共赢" class="headerlink" title="6.合作共赢"></a>6.合作共赢</h1><p>寻找合作中的交叉共赢点，把生活、工作、学习当做是合作方。</p><p>寻找共赢点，达到双方共赢的目的</p><h1 id="7-学历-lt-gt-学习力"><a href="#7-学历-lt-gt-学习力" class="headerlink" title="7.学历&lt;&gt;学习力"></a>7.学历&lt;&gt;学习力</h1><p>学历并不等于学习力，继续学习没有什么不好</p><h1 id="8-信任和欣赏"><a href="#8-信任和欣赏" class="headerlink" title="8.信任和欣赏"></a>8.信任和欣赏</h1><p>没人身上都有值得欣赏的地方，每个人都会向被欣赏的方向努力</p><p>当批评的方式不管用的时候可以试试使用欣赏的方式</p><p>欣赏的三要素：及时 、具体 、发自内心</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（四）</title>
      <link href="undefined2018/10/10/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>2018/10/10/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="沟通"><a href="#沟通" class="headerlink" title="沟通"></a>沟通</h1><p>沟通是以解决问题，达成一致的目标为方向的</p><p>因此，所有场合的沟通，是需要肯定对方。</p><p>所有的沟通都一肯定对方为主</p><h2 id="沟通批评的三步走法则"><a href="#沟通批评的三步走法则" class="headerlink" title="沟通批评的三步走法则"></a>沟通批评的三步走法则</h2><ul><li>肯定 先肯定对方</li><li>批评 在就事论事，高表扬 ，低批评</li><li>信任 信任对方</li></ul><h1 id="管理学"><a href="#管理学" class="headerlink" title="管理学"></a>管理学</h1><h2 id="管理的重心"><a href="#管理的重心" class="headerlink" title="管理的重心"></a>管理的重心</h2><p>管理的重心不应该是工作和物质上的因素，应该把目光转到人的因素上去，人被重视了以后，就会更加卖力，做的更加出色</p><p>目标的管理并不是管理目标，而是管理从目标到得到成果的全过程</p><h1 id="目标的制定"><a href="#目标的制定" class="headerlink" title="目标的制定"></a>目标的制定</h1><h2 id="目的目标计划"><a href="#目的目标计划" class="headerlink" title="目的目标计划"></a>目的目标计划</h2><ul><li>目的：努力的方向</li><li>目标：预期的结果</li><li>计划：为实现目标和目的，设定安排好 责任人、实施方法、时间日程</li></ul><p>做好目标，考虑做什么样的人，成就什么样的事业，才会向着自己的方向前进</p><h2 id="制定目标的好处"><a href="#制定目标的好处" class="headerlink" title="制定目标的好处"></a>制定目标的好处</h2><ul><li>目标是行动的基础、导航，为行动指明方向</li><li>有目标才能有效的利用资源，不会漫无目的</li><li>达成目标后，有成就感，有激励的作用</li></ul><h2 id="企业的绩效需要从四个方面来衡量"><a href="#企业的绩效需要从四个方面来衡量" class="headerlink" title="企业的绩效需要从四个方面来衡量"></a>企业的绩效需要从四个方面来衡量</h2><p>企业的绩效不能只从财务指标角度来衡量考虑</p><p>应该从财务、客户、内部流程、学习与成长 四个层面来衡量企业的业绩</p><h2 id="目标的制定-1"><a href="#目标的制定-1" class="headerlink" title="目标的制定"></a>目标的制定</h2><ul><li>高层基于战略通过SWOT分析指定整体目标</li><li>遵循SMART原则</li><li>目标层级清晰</li><li>目标数量3-5个</li><li>由上而下 基于事实 逐层分解 参与制定 忌过高过低 激发实现意愿</li><li>目标对话 （目标意义、预测困难、解决方案、适当授权、明确奖惩、提出支持）</li></ul><h2 id="SWOT分析"><a href="#SWOT分析" class="headerlink" title="SWOT分析"></a>SWOT分析</h2><ol><li>Strangths 优势</li><li>Weaknesses 劣势</li><li>Threats 威胁</li><li>Opportunities 机会</li></ol><p>通过分析：</p><ul><li>找出有利的、值得发扬的因素，找出不利的、要避免的因素，发现问题，找出解决方案</li><li>根据轻重缓急或影响程度等一一列出</li><li>根据战略目标的基本思路 客服各种因素</li></ul><h2 id="目标管理SMART原则"><a href="#目标管理SMART原则" class="headerlink" title="目标管理SMART原则"></a>目标管理SMART原则</h2><ul><li>明确的</li><li>可度量的</li><li>可达到的</li><li>相关联的</li><li>有时限的</li></ul><h2 id="目标结构化分解的要点"><a href="#目标结构化分解的要点" class="headerlink" title="目标结构化分解的要点"></a>目标结构化分解的要点</h2><ul><li>某项任务的内容时旗下所有任务的总和</li><li>各项任务应该区分轻重缓急</li><li>一项任务只能由一个人负责，及时许多人都可能为其工作，也只能有一个人负责，其他人只能是参与者</li><li>应该让团队成员积极参与目标分解，以确保达成共识</li><li>每项任务最终要文档化，以确保准确理解和跟踪</li><li>每项任务应该有预案和弥补措施</li><li>通常层次不超过10层建议4-6层</li></ul><h2 id="下达任务要素"><a href="#下达任务要素" class="headerlink" title="下达任务要素"></a>下达任务要素</h2><ol><li>what</li><li>why</li><li>who</li><li>when</li><li>where</li><li>how</li><li>how much</li></ol><p>下达任务：必须交代的问题 1 ，3 ，4 ，7</p><h2 id="授权与交责"><a href="#授权与交责" class="headerlink" title="授权与交责"></a>授权与交责</h2><p>谁的任务谁来管，照顾好自己的任务</p><p>不要出现没人管理的任务</p><p>管理者应该培养下属管理好自己的任务</p><p>下属应该提出选择题，而不是问题</p><p>管理者不要当“家长”、“保姆”</p><p>教练式启发，引发下属思考</p><p>及时鼓励和肯定</p><p>恰当授权，让下属担当（锁定责任人）</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（一）</title>
      <link href="undefined2018/08/06/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>2018/08/06/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="1-知识体系"><a href="#1-知识体系" class="headerlink" title="1.知识体系"></a>1.知识体系</h1><p>MTP综合管理技能 知识体系 ：① 管理心理学 ②九型人格 ③神经语言程序学 ④教练技术 ⑤管理培训计划</p><h1 id="2-管理技能架构"><a href="#2-管理技能架构" class="headerlink" title="2.管理技能架构"></a>2.管理技能架构</h1><p>对于自己（角色，心态）：管理好自己的角色和心态<br>对于人（培养，沟通，激励）：做好教练，教练（教练技术一项通过改善被教练者心智模式来发挥其潜能和提升效率的管理技术）<br>对于事（计划，指挥，控制）：顾问 给予做事的方法</p><h1 id="3-缺点，盲点"><a href="#3-缺点，盲点" class="headerlink" title="3.缺点，盲点"></a>3.缺点，盲点</h1><p>失败是发现自己盲点的绝佳机会，失败中自我反思失败原因</p><h1 id="4-优秀管理者的素质模型"><a href="#4-优秀管理者的素质模型" class="headerlink" title="4.优秀管理者的素质模型"></a>4.优秀管理者的素质模型</h1><p>①领袖素质（决定管理层高低）：激励 负责任 信任 共赢<br>②基本管理能力：科学决策 授权 沟通 目标方向<br>③人力资源管理能力（非人力资源管理技能）：选育用留<br>④业务管理能力:专业知识和业务管理能力</p><h1 id="5-信念"><a href="#5-信念" class="headerlink" title="5.信念"></a>5.信念</h1><p>信念决定人生高度，扩宽自己的信念，则会有更大的成果 </p><h1 id="6-素质领袖"><a href="#6-素质领袖" class="headerlink" title="6.素质领袖"></a>6.素质领袖</h1><p>发挥自己的优势，弥补自己最大的缺陷</p><h1 id="7-称职？"><a href="#7-称职？" class="headerlink" title="7.称职？"></a>7.称职？</h1><p>何为称职  人总是处于有挑战的位置，每个人的职位应为 20%的挑战 + 80%的把握  </p><p>因此，需要①包容上司处于不胜任的状态②接纳自己的不足③洞察下属是否有超胜任及超不胜任的情况</p><h1 id="8-探秘术"><a href="#8-探秘术" class="headerlink" title="8.探秘术"></a>8.探秘术</h1><p>凡事，先“横推”，后“下切”</p><h1 id="9-管理者角色的认知"><a href="#9-管理者角色的认知" class="headerlink" title="9.管理者角色的认知"></a>9.管理者角色的认知</h1><p>企业中的管理者是经营者的替身，考虑和看待问题需要从经营者的一直和利益出发，从经营者的角度看待整个问题</p><h1 id="10-着眼当前，放眼未来"><a href="#10-着眼当前，放眼未来" class="headerlink" title="10.着眼当前，放眼未来"></a>10.着眼当前，放眼未来</h1><p>数据分析当前的状态，综合现在的管理制度，优化和改善现在的制度</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zabbix Server 配置优化</title>
      <link href="undefined2017/09/22/Zabbix-Server-%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
      <url>2017/09/22/Zabbix-Server-%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>Zabbix 是一款性能特别高的一款分布式监控报警系统，在平时的运行维护当中，是一款不错的监控软件，可以实施监控服务器以及各种设备设施的状态。</p><h2 id="Zabbix-简单的优化调整"><a href="#Zabbix-简单的优化调整" class="headerlink" title="Zabbix 简单的优化调整"></a>Zabbix 简单的优化调整</h2><p>由于监控设备的数量是不定的，有些时候因为默认参数配置的不够，会导致Zabbix自身状态不稳定，导致报警。比如说：</p><p><code>Zabbix unreachable poller processes more than 75% busy</code></p><p><code>Zabbix discoverer processes more than 75% busy</code></p><p>因此，为了Zabbix运行的更加稳定，我们就需要对Zabbix做相应的优化处理</p><h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><p><font color="pink">这部分很重要</font></p><p>下面是我们监控项中所有项目所对应的参数配置。</p><p>关于 Zabbix data gathering process busy % 参数</p><pre><code>Zabbix busy trapper processes, in %                     StartTrappers=Zabbix busy poller processes, in %                      StartPollers=Zabbix busy ipmi poller processes, in %                 StartIPMIPollers=Zabbix busy discoverer processes, in %                  StartDiscoverers=Zabbix busy icmp pinger processes, in %                 StartPingers=Zabbix busy http poller processes, in %                 StartHTTPPollers=Zabbix busy proxy poller processes, in %                StartProxyPollers=Zabbix busy unreachable poller processes, in %          StartPollersUnreachable=Zabbix busy java poller processes, in %                 StartJavaPollers=Zabbix busy snmp trapper processes, in %                StartSNMPTrapper=Zabbix busy vmware collector processes, in %            StartVMwareCollectors=</code></pre><p>关于 Zabbix cache usage 参数</p><pre><code>Zabbix-server: Zabbix trend write cache, % free         TrendCacheSize=Zabbix-server: Zabbix configuration cache, % free       CacheSize=Zabbix-server: Zabbix text write cache, % free          HistoryTextCacheSize=Zabbix-server: Zabbix history write cache, % free       HistoryCacheSize=Zabbix-server: Zabbix value cache, % free               ValueCacheSize=Zabbix-server: Zabbix vmware cache, % free              VMwareCacheSize=</code></pre><h4 id="调整优化"><a href="#调整优化" class="headerlink" title="调整优化"></a>调整优化</h4><p>在监控的时候，Zabbix 监控出现性能瓶颈的时候会发生警报，我们可以根据警报项，对我们 Zabbix的配置文件做相应的调整，对Zabbix做相应的优化，使之适应我们需要的工作环境。</p><p>下面我们是使用一个报警参数来进行解释说明。</p><p>报警信息：Zabbix unreachable poller processes more than 75% busy </p><p>按照信息，我们可以得出，报警信息所对应的配置参数为<code>StartProxyPollers</code> ，我们配置文件中此参数所默认的参数值为 1，我们可以对此参数做相应的提高，以提高我们Zabbix的性能。</p><p>步骤如下：</p><p>1.找到我们Zabbix服务端安装路径。并在安装目录下找到服务器端配置文件 <font color="green">zabbix_server.conf</font> ，例如我的配置文件目录如下，可以参考。</p><pre><code class="bash">$ cd /usr/local/zabbix/etc/</code></pre><p>2.修改配置文件参数<font color="green">(文件名可能会有差异，差异在于中间的下划线，请勿直接拷贝)</font></p><pre><code class="bash">$ vim zabbix_server.conf</code></pre><p>在默认的<code># StartProxyPollors = 1</code>下方添加一条配置文件</p><p><code>StartProxyPollors = 5</code> <font color="green">//参数可以根据自己需要增大，寻找自己环境适合的参数值</font></p><p>3.重启zabbix服务，使刚才修改的配置文件生效。</p><pre><code class="bash">$ service zabbix_server restart</code></pre><p>回到监控系统中看看效果吧</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>