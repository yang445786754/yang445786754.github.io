<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RDS-MYSQL事务锁及解决办法</title>
      <link href="undefined2020/07/11/RDS-MYSQL%E4%BA%8B%E5%8A%A1%E9%94%81%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
      <url>2020/07/11/RDS-MYSQL%E4%BA%8B%E5%8A%A1%E9%94%81%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>个人理解，如有错误，请予指出</p><h1 id="介绍·场景"><a href="#介绍·场景" class="headerlink" title="介绍·场景"></a>介绍·场景</h1><p>先解释一下事务，事务是有用户定义的一个服务操作队列，由MYSQL服务器保证这些操作队列再多个客户端并发访问和服务器出现故障时的原子性。</p><p>锁，是用来再多个事务访问同一个对象时，根据这些操作访问同一个对象的向后执行顺序，给事务进行排序执行。当一个事务获取到锁以后，其他的事务将会等待锁的释放，才能继续执行。</p><p>MySQL在进行<code>alter table</code>表结构等改变情况下的DDL操作时，出现大量的<code>Waiting for table metadata lock</code>的等待队列。当执行语句<code>alter table Table</code>之后，后续对Table表的任何操作（包括select操作）都无法进行,将会影响到生产。</p><p>可能造成锁等待的原因：</p><ol><li><p>需要很长执行时间的长事务运行，阻塞了所有的DDL语句，</p></li><li><p>因连接中断等额外因素未按照预期提交事务，当事务执行完毕，但未提交或回滚，而造成的DLL事务阻塞，造成锁表，</p></li><li><p>官方文档中有说明，除了语法错误的情况，其他错误语句（如查询不存在的列）获取到的锁，在这个事务提交或者回滚之前，锁不会被释放。</p></li></ol><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>先介绍几个表：</p><ul><li><p>information_schema.innodb_trx 当前运行的所有事务</p></li><li><p>information_schema.innodb_locks 当前出现的锁</p></li><li><p>information_schema.innodb_lock_waits 锁表等待的对应关系表</p></li><li><p>performance_schema.events_statements_current 当前时刻每个Session会话正在执行的SQL语句</p></li><li><p>information_schema.processlist 当前正在运行的线程，root 用户可见所有用户的线程</p></li></ul><p>可以使用语句<code>show processlist</code> (也可使用select语句)先查看当前所有用户所有线程的状态，如有大量的线程执行语句处于<code>Waiting for table metadata lock</code> 状态，排除硬件性能造成的等待外，或确定正在操作特长时间的事务，极有可能已经造成锁表导致之后的DLL语句阻塞，</p><pre><code class="sql">mysql&gt; show processlist;</code></pre><p>查询 innodb_trx 获取事务的信息，可以查看当前事务的信息包括所执行的语句、所需要获取的锁、所对应processlist线程表中的ID等信息</p><pre><code class="sql">mysql&gt; select * from information_schema.innodb_trx;</code></pre><p>查询 innodb_locks 获取当前出现的锁情况，</p><pre><code class="sql">mysql&gt; select * from information_schema.innodb_locks;</code></pre><p>查询 innodb_lock_waits 可以获取到目前正在等待获取锁的事务ID，及正在使用锁的ID。</p><pre><code class="sql">mysql&gt; select * from information_schema.innodb_lock_waits;</code></pre><p>我们收集到了上述的信息，如果确定存在异常锁表，我们就可以根据表、查询语句、事务反向查找到正在执行该语句的线程。通过其线程ID将其干掉，马桶就通了。</p><pre><code class="sql">mysql&gt; kill tread_id;</code></pre>]]></content>
      
      
      <categories>
          
          <category> RDS </category>
          
          <category> MYSQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDS </tag>
            
            <tag> MYSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构-队列(Queue)</title>
      <link href="undefined2019/09/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97-Queue/"/>
      <url>2019/09/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97-Queue/</url>
      
        <content type="html"><![CDATA[<p>队列（Queue）是只允许在一端进行插入操作，而在另一端进行删除操作的线性表</p><p>队列是一种先进先出，First in First out ,FIFO的线性表，允许一端插入，另一端删除输出，而不允许在中间位置操作。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>我通过顺序list实现，会产生入队和出队的时间复杂度问题,通过实际问题考虑后决定</p><ul><li>后入头出<ul><li>入队时间复杂度为O(1)</li><li>出队时间复杂度为O(n)</li></ul></li><li>头入后出<ul><li>入队时间复杂度为O(n)</li><li>出队时间复杂度为O(1)</li></ul></li></ul><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : Queue.py# @Time : 2019/9/24 11:28# @Author : Tony_9410# @contact: tony_9410@foxmail.com# @Software: PyCharmclass Queue:    &quot;&quot;&quot;队列&quot;&quot;&quot;    def __init__(self):        self.__list = []    def enqueue(self,item):        &quot;&quot;&quot;入队&quot;&quot;&quot;        self.__list.append(item)        # self.__list.insert(0,item)    def dequeue(self):        &quot;&quot;&quot;出队&quot;&quot;&quot;        return self.__list.pop(0)        # return self.__list.pop()    def is_empty(self):        return not self.__list    def size(self):        return len(self.__list)if __name__ == &#39;__main__&#39;:    q = Queue()    for i in range(10):        q.enqueue(i)    for i in range(10):        print(q.dequeue())</code></pre><h1 id="双端队列"><a href="#双端队列" class="headerlink" title="双端队列"></a>双端队列</h1><p>两端均可增可取</p><pre><code class="python">class Deque:    &quot;&quot;&quot;双端队列&quot;&quot;&quot;    def __init__(self):        self.__list = []    def add_front(self,item):        self.__list.insert(0,item)    def add_rear(self,item):        self.__list.append(item)    def pop_front(self):        return self.__list.pop(0)    def pop_rear(self):        return self.__list.pop()    def is_empty(self):        return not self.__list    def size(self):        return len(self.__list)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构-栈(Stack)</title>
      <link href="undefined2019/09/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88-Stack/"/>
      <url>2019/09/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88-Stack/</url>
      
        <content type="html"><![CDATA[<p>栈(Stack)，也称为堆栈，是一种容器，可存入数据元素、访问元素、删除元素。特点是：仅允许在栈的一端（栈顶）做数据操作，没有位置上的概念，保证在任何时候操作数据都是最近时间存入的那个数据。</p><p>该数据结构只允许在一端进行操作，因此按照后进先出，LIFO，last in first out 的原理操作</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>实现也可以通过其他结构实现，例子通过列表实现</p><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : Stack.py# @Time : 2019/9/24 11:10 # @Author : Tony_9410# @contact: tony_9410@foxmail.com# @Software: PyCharmclass Stack:    &quot;&quot;&quot;栈&quot;&quot;&quot;    def __init__(self):        self.__list = []    def push(self,item):        &quot;&quot;&quot;压栈&quot;&quot;&quot;        self.__list.append(item)    def pop(self):        &quot;&quot;&quot;出栈&quot;&quot;&quot;        return self.__list.pop()    def peek(self):        &quot;&quot;&quot;返回元素，不出栈&quot;&quot;&quot;        if self.__list:            return self.__list[-1]        else:            return None    def is_empty(self):        return not self.__list    def size(self):        return len(self.__list)if __name__ == &#39;__main__&#39;:    s = Stack()    for i in range(10):        s.push(i)    for i in range(10):        print(s.pop())</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构-Python变量的本质</title>
      <link href="undefined2019/09/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-Python%E5%8F%98%E9%87%8F%E7%9A%84%E6%9C%AC%E8%B4%A8/"/>
      <url>2019/09/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-Python%E5%8F%98%E9%87%8F%E7%9A%84%E6%9C%AC%E8%B4%A8/</url>
      
        <content type="html"><![CDATA[<p>Python中和其他语言不同的，没有地址这个专属数据类型的，如C中添加*号则表示地址，而在Python这门高级语言中，是没有地址这个数据类型的。</p><h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1><p>在其他语言中如果需要保存一个int类型的变量，如<code>a = 5</code>,就是将数据5保存在内存当中，而<code>a</code>就是这部分内存的别名。</p><p>而在python当中，在内存中保存数值5后，再在内存中保存一个a的值，a值保存的是数值5的内存地址。</p><p>如果在交换变量的时候。仅仅需要将a，b保存的内存地址进行交换，如<code>a = 5;b = 10;a,b = b,a</code>,就是说，ab在内存中所存储的数据并不是真正的数值，而是数值所在的地址。</p><p>就因为这一点，Python 语言中的变量可以指向任意对象的地址，因此，就是我们所看到的，可以把一个函数赋值给予一个变量。</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构-链表</title>
      <link href="undefined2019/09/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8/"/>
      <url>2019/09/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 相比于线性表顺序结构，操作复杂。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而线性表和顺序表相应的时间复杂度分别是O(logn)和O(1)。</p><p>我每次都会先在百度百科上搜下定义</p><p>链表和顺序表一样，是顺序表，但不是顺序存储的结构，在每个节点（数据存储单元）里存放下一个节点的位置信息（内存地址）。</p><p>链表可以充分利用计算机内存空间，实现灵活的内存动态管理</p><h1 id="单向链表"><a href="#单向链表" class="headerlink" title="单向链表"></a>单向链表</h1><p>每个节点包含两个域，信息域（元素域）和链接域。尾节点的链接域为空</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>使用python代码实现，并实现部分操作功能，链表是为了更灵活的管理零散非连续的内存空间。</p><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : 单向链表.py# @Time : 2019/9/23 14:20# @Author : Tony_9410# @contact: tony_9410@foxmail.com# @Software: PyCharmclass Node:    &quot;&quot;&quot;节点&quot;&quot;&quot;    def __init__(self,item,next = None):        # 数据        self.item = item        # 指向下一个节点        self.next = nextclass SingleNodeList:    &quot;&quot;&quot;单向链表&quot;&quot;&quot;    def __init__(self,node = None):        # 指向头节点        self.__head = node    def is_empty(self):        &quot;&quot;&quot;返回是否为空&quot;&quot;&quot;        return self.__head == None    def length(self):        &quot;&quot;&quot;返回长度&quot;&quot;&quot;        # 游标指向__head，逐个向后并计数，判断最后的next是否为空，为空则退出        cur = self.__head        count = 0        while cur:            count +=1            cur =cur.next        return count    def travel(self):        &quot;&quot;&quot;遍历整个列表&quot;&quot;&quot;        cur = self.__head        while cur:            print(cur.item,end=&quot;-&gt;&quot;)            cur = cur.next        print(&quot;None&quot;)    def add(self,item):  # O(1)        &quot;&quot;&quot;链表头部添加元素&quot;&quot;&quot;        node = Node(item)        node.next = self.__head        self.__head = node    def append(self,item):  # O(n)        &quot;&quot;&quot;链表尾部条件元素&quot;&quot;&quot;        node = Node(item)        if self.is_empty():            self.__head = node        else:            cur = self.__head            while cur.next:                cur = cur.next            cur.next = node    def insert(self,pos,item):  # O(n)        &quot;&quot;&quot;指定位置添加元素        :param pos 从0开始        :param item 添加的元素        &quot;&quot;&quot;        if pos &lt;= 0:            self.add(item)        elif pos &gt; (self.length() -1):            self.append(item)        else:            pre = self.__head            node = Node(item)            while pos-1:                pre = pre.next                pos -= 1            node.next = pre.next            pre.next = node    def remove(self,item):        &quot;&quot;&quot;删除节点&quot;&quot;&quot;        cur = self.__head        pre = None        while cur:            if cur.item == item:                if pre == None:                    self.__head = cur.next                else:                    pre.next = cur.next                break            else:                pre = cur                cur = cur.next    def search(self,item):  # O(n)        &quot;&quot;&quot;搜索节点 遍历列表返回元素是否存在&quot;&quot;&quot;        cur = self.__head        while cur:            if cur.item == item:                return True            else:                cur = cur.next        return Falseif __name__ == &#39;__main__&#39;:    snl = SingleNodeList()    print(&quot;是否为空&quot;, snl.is_empty())    print(&quot;lenth:&quot;, snl.length())    snl.append(&quot;宇宙&quot;)    snl.append(&quot;地球&quot;)    snl.append(&quot;世界&quot;)    snl.append(&quot;中国&quot;)    snl.append(&quot;北京&quot;)    snl.append(&quot;丰台&quot;)    snl.insert(6,&quot;乱入&quot;)    print(&quot;是否为空&quot;,snl.is_empty())    print(&quot;lenth:&quot;,snl.length())    snl.travel()    snl.remove(&quot;乱入&quot;)    snl.travel()</code></pre><h1 id="单向循环链表"><a href="#单向循环链表" class="headerlink" title="单向循环链表"></a>单向循环链表</h1><p>单向循环链表就是把尾节点指向头节点，形成循环</p><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : 单向循环链表.py# @Time : 2019/9/24 08:20# @Author : Tony_9410# @contact: tony_9410@foxmail.com# @Software: PyCharmclass Node:    &quot;&quot;&quot;节点&quot;&quot;&quot;    def __init__(self,item,next = None):        # 数据        self.item = item        # 指向下一个节点        self.next = nextclass SingleCycleNodeList:    &quot;&quot;&quot;单向链表&quot;&quot;&quot;    def __init__(self,node = None):        self.__head = node        # 将尾节点指向头节点        if node:            node.next = node    def is_empty(self):        &quot;&quot;&quot;返回是否为空&quot;&quot;&quot;        return self.__head == None    def length(self):        &quot;&quot;&quot;返回长度&quot;&quot;&quot;        # 游标指向__head，逐个向后并计数，判断最后的next是否为空，为空则退出        if self.is_empty():            return 0        cur = self.__head        count = 1        while cur.next != self.__head:            count +=1            cur =cur.next        return count    def travel(self):        &quot;&quot;&quot;遍历整个列表&quot;&quot;&quot;        if self.is_empty():            return        cur = self.__head        while cur.next != self.__head:            print(cur.item,end=&quot;-&gt;&quot;)            cur = cur.next        print(cur.item)    def add(self,item):          &quot;&quot;&quot;链表头部添加元素&quot;&quot;&quot;        node = Node(item)        if self.__head == None:            self.__head = node            node.next = self.__head        cur = self.__head        while cur.next != self.__head:            cur = cur.next        node.next = self.__head        self.__head = node        cur.next = self.__head    def append(self,item):          &quot;&quot;&quot;链表尾部条件元素&quot;&quot;&quot;        node = Node(item)        if self.is_empty():            self.__head = node            node.next = self.__head        else:            cur = self.__head            while cur.next != self.__head:                cur = cur.next            node.next = self.__head            cur.next = node    def insert(self,pos,item):          &quot;&quot;&quot;指定位置添加元素        :param pos 从0开始        :param item 添加的元素        &quot;&quot;&quot;        if pos &lt;= 0:            self.add(item)        elif pos &gt; (self.length() -1):            self.append(item)        else:            pre = self.__head            node = Node(item)            while pos-1:                pre = pre.next                pos -= 1            node.next = pre.next            pre.next = node    def remove(self, item):        &quot;&quot;&quot;删除节点&quot;&quot;&quot;        if self.is_empty():            return        cur = self.__head        pre = None        while cur.next != self.__head:            if cur.item == item:                if cur == self.__head:                    # 头节点                    rear = self.__head                    while rear.next != self.__head:                        rear = rear.next                    self.__head = self.__head.next                    rear.next = self.__head                else:                    pre.next = cur.next                return            else:                pre = cur                cur = cur.next        # 尾节点        if cur.item == item:            # 仅有单个节点            if cur == self.__head:                self.__head = None            else:                pre.next = cur.next    def search(self, item):          &quot;&quot;&quot;搜索节点 遍历列表返回元素是否存在&quot;&quot;&quot;        if self.is_empty():            return False        cur = self.__head        while cur:            if cur.item == item:                return True            else:                cur = cur.next        if cur.item == item:            return True        return False    def testcycle(self):        cur = self.__head        if cur == None:            return 0        while True:            yield cur.item            cur = cur.nextif __name__ == &#39;__main__&#39;:    snl = SingleCycleNodeList()    print(&quot;是否为空&quot;, snl.is_empty())    print(&quot;lenth:&quot;, snl.length())    snl.append(&quot;宇宙&quot;)    snl.append(&quot;地球&quot;)    snl.append(&quot;世界&quot;)    snl.append(&quot;中国&quot;)    snl.append(&quot;北京&quot;)    snl.append(&quot;丰台&quot;)    snl.insert(0,&quot;乱入&quot;)    print(&quot;是否为空&quot;,snl.is_empty())    print(&quot;lenth:&quot;,snl.length())    snl.travel()    snl.remove(&quot;乱入&quot;)    snl.travel()    # 测试循环    i = 10    cycle = snl.testcycle()    while i:        print(next(cycle))        i-=1</code></pre><h1 id="双向链表"><a href="#双向链表" class="headerlink" title="双向链表"></a>双向链表</h1><p>双向链表中节点存储链接存储上一个节点的内存地址。可以通过单个节点找到上一个节点和下一个节点。</p><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : 双向链表.py# @Time : 2019/9/24 14:20# @Author : Tony_9410# @contact: tony_9410@foxmail.com# @Software: PyCharmclass Node:    &quot;&quot;&quot;节点&quot;&quot;&quot;    def __init__(self,item,prev = None,next = None):        # 指向前一个节点        self.prev = prev        # 数据        self.item = item        # 指向下一个节点        self.next = nextclass DoubleNodeList:    &quot;&quot;&quot;双向链表&quot;&quot;&quot;    def __init__(self,node = None):        # 指向头节点        self.__head = node    def is_empty(self):        &quot;&quot;&quot;返回是否为空&quot;&quot;&quot;        return self.__head is None    def length(self):        &quot;&quot;&quot;返回长度&quot;&quot;&quot;        # 游标指向__head，逐个向后并计数，判断最后的next是否为空，为空则退出        cur = self.__head        count = 0        while cur:            count +=1            cur =cur.next        return count    def travel(self):        &quot;&quot;&quot;遍历整个列表&quot;&quot;&quot;        cur = self.__head        print(&quot;None&quot;,end=&quot;&lt;-&gt;&quot;)        while cur:            print(cur.item,end=&quot;&lt;-&gt;&quot;)            cur = cur.next        print(&quot;None&quot;)    def add(self,item):  # O(1)        &quot;&quot;&quot;链表头部添加元素&quot;&quot;&quot;        node = Node(item)        if self.is_empty():            self.__head = node        else:            node.next = self.__head            self.__head.prev = node            self.__head = node    def append(self,item):  # O(n)        &quot;&quot;&quot;链表尾部条件元素&quot;&quot;&quot;        node = Node(item)        if self.is_empty():            self.__head = node        else:            cur = self.__head            while cur.next:                cur = cur.next            node.prev = cur            cur.next = node    def insert(self,pos,item):  # O(n)        &quot;&quot;&quot;指定位置添加元素        :param pos 从0开始        :param item 添加的元素        &quot;&quot;&quot;        if pos &lt;= 0:            self.add(item)        elif pos &gt; (self.length() -1):            self.append(item)        else:            node = Node(item)            cur = self.__head            while pos:                cur = cur.next                pos -= 1            node.next = cur            node.prev = cur.prev            cur.prev = node            cur.prev.next = node    def remove(self,item):        &quot;&quot;&quot;删除节点&quot;&quot;&quot;        cur = self.__head        while cur:            if cur.item == item:                if cur == self.__head:                    #头节点                    self.__head = cur.next                    if cur.next:                        # 单节点                        cur.next.prev = None                else:                    cur.prev.next = cur.next                    if cur.next:                        # 尾节点                        cur.next.prev = cur.prev                break            else:                cur = cur.next    def search(self,item):  # O(n)        &quot;&quot;&quot;搜索节点 遍历列表返回元素是否存在&quot;&quot;&quot;        cur = self.__head        while cur:            if cur.item == item:                return True            else:                cur = cur.next        return Falseif __name__ == &#39;__main__&#39;:    dnl = DoubleNodeList()    print(&quot;是否为空&quot;, dnl.is_empty())    print(&quot;lenth:&quot;, dnl.length())    dnl.append(&quot;地球&quot;)    dnl.append(&quot;世界&quot;)    dnl.append(&quot;中国&quot;)    dnl.append(&quot;北京&quot;)    dnl.append(&quot;丰台&quot;)    dnl.add(&quot;宇宙&quot;)    dnl.insert(0,&quot;乱入&quot;)    print(&quot;是否为空&quot;,dnl.is_empty())    print(&quot;lenth:&quot;,dnl.length())    dnl.travel()    dnl.remove(&quot;乱入&quot;)    dnl.travel()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法-算法</title>
      <link href="undefined2019/09/12/%E7%AE%97%E6%B3%95-%E7%AE%97%E6%B3%95/"/>
      <url>2019/09/12/%E7%AE%97%E6%B3%95-%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>算法：即计算机处理信息的本质。解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。</p><h1 id="算法的特性"><a href="#算法的特性" class="headerlink" title="算法的特性"></a>算法的特性</h1><ol><li>输入</li><li>输出</li><li>有穷性：算法在有限步骤之内结束，而不是无线循环</li><li>可行性：该方法可以被实现，在有限时间内完成</li><li>确切性：每个步骤有确切的定义</li></ol><h1 id="算法衡量"><a href="#算法衡量" class="headerlink" title="算法衡量"></a>算法衡量</h1><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>执行算法所需要的计算工作量。百度百科上的定义：一般来说，计算机算法是问题规模n 的函数f(n)，算法的时间复杂度也因此记做。<br><code>T(n)=Ο(f(n))</code><br>因此，问题的规模n 越大，算法执行的时间的增长率与f(n) 的增长率正相关，称作渐进时间复杂度（Asymptotic Time Complexity）。</p><h3 id="计算时间复杂度"><a href="#计算时间复杂度" class="headerlink" title="计算时间复杂度"></a>计算时间复杂度</h3><ol><li>基本操作，只有常数项，认为时间复杂程度为O(1)</li><li>顺序结构，时间复杂度按照加法进行计算</li><li>循环结构，时间复杂程度按照乘法计算</li><li>分支结构，时间复杂程度按照取最大值计算</li><li>取最高次项作为时间复杂度</li><li>无特殊说明时间复杂度均为最坏时间复杂度</li></ol><h3 id="评判"><a href="#评判" class="headerlink" title="评判"></a>评判</h3><p><code>O(1)&lt;O(logn)&lt;O(n)&lt;O(nlogn)&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)&lt;O(n!)&lt;O(n^n)</code></p><h3 id="python-内置时间分析"><a href="#python-内置时间分析" class="headerlink" title="python 内置时间分析"></a>python 内置时间分析</h3><p>timeit模块测试</p><pre><code class="python">from timeit import Timertimer = Timer(&quot;func()&quot;,&quot;from __main__ import func&quot;)timer.timeit(number = 1000)</code></pre><h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h2><p>算法的空间复杂度是指算法需要消耗的内存空间。其计算和表示方法与时间复杂度类似，一般都用复杂度的渐近性来表示。</p><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-Data Compression 数据压缩降维</title>
      <link href="undefined2019/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Data-Compression-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E9%99%8D%E7%BB%B4/"/>
      <url>2019/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Data-Compression-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>通常我们的数据集的特征会有很多，有部分特征会产生冗余，可以使用降维的方式去降低特征的维度，将3D的特征降维成为2D将2D降低成为1D。想法是，在避免丢失信息的情况下将多维度数据投影到一条直线或一个平面上，这条直线并不是坐标轴。这样可以加快我们后面算法的运算速度。</p><p><img src="datacompression.png" alt="kmeanoptimization"></p><h1 id="主成分"><a href="#主成分" class="headerlink" title="主成分"></a>主成分</h1><p>主成分分析可以在训练集中识别出那一条轴对差异性的贡献度最高，</p><h2 id="如何找到主成分"><a href="#如何找到主成分" class="headerlink" title="如何找到主成分"></a>如何找到主成分</h2><p>有一种标准举证分解技术，叫做奇异值分解。他会将训练集矩阵X分解成为三个矩阵的点积U·Sigma·V^T,其中V^T包含我们所想要的所有主成分</p><h2 id="Principal-Componet-Analysis-PCA"><a href="#Principal-Componet-Analysis-PCA" class="headerlink" title="Principal Componet Analysis (PCA)"></a>Principal Componet Analysis (PCA)</h2><ul><li>获取数据x1,x2,x3…xm</li><li>先对数据进行均值归一化,计算均值，然后使每个样本数据减去均值，使用xi - uj,uj为均值<!-- u_j = \frac1m \sum_{i=1}^{m}x_j^{(i)} --><img src="meanscalar.png" alt="meanscalar"></li><li>在低维子空间中寻找一个向量或者一个面来投影我们所有的数据点<ul><li>计算”协方差”convariance matrix 获取主成分  <!-- \Sigma = \frac1m \sum_{i = 1}^{n}(x^{(i)})(x^{(i)})^T -->  <img src="computeconvariancematrix.png" alt="computeconvariancematrix"></li><li>我们可以使用munpy中的svd()函数获取训练集中的所有主成分，下列示例我们获取主成分中的前两个主成分</li></ul></li></ul><pre><code class="python">X_centered = X - X.mean(axis = 0)U,s,V = np.linalg.svd(X_centered)c1 = V.T[:,0]c2 = V.T[:,1]</code></pre><ul><li>我们已经确定获取到了所有主成分，就可以将数据集投影到前d个主成分定义的超平面上从而将数据集维度从n维度，降低到了d个维度，需要将数据集投影到前d个主成分上只需要将训练集与前d个主成分组成的矩阵点积即可。<ul><li>X_{d-proj} = X · Wd      ；Wd为前d个主成分组合矩阵</li></ul></li></ul><pre><code class="python">W2 = V.T[:,:2]X2D = X_centered.dot(W2)</code></pre><p>这样实现了在尽可能保留多差异性的同时，将任意数据集降低到任意维度</p><p>另：sklean中的PCA</p><pre><code class="python">from sklean.decomposition import PCApca = PCA(n_components = 2)X2D = pca.fit_transform(X)</code></pre><p>通过<code>pca.components_</code>获取主成分例如获取第一个主成分<code>pca.components_.T[:,0]</code></p><h1 id="方差解释率"><a href="#方差解释率" class="headerlink" title="方差解释率"></a>方差解释率</h1><p>非常有用的主成分信息是主成分的方差解释率。他表示的是每个主成分对数据集方差的贡献程度，从这个角度上来说，我们在K维的数据集上，我们保留主成分时候，只要保留的方差保持在90以上，我们的数据降维就是可行的。方差结实率在sklean中可以通过<code>pca.explained_variance_ratio_</code>获取</p><p>在sklean中我们可以直接设置保留多少方差，来获取最低维度数量。</p><pre><code class="python">pca = PCA()pca.fit(X)cumsum = np.cumsum(pca.explained_variance_ratio_)d = np.argmax(cumsum &gt;=0.95) + 1</code></pre><p>d就是我们获得的最低维度数量，我们再次使用d来执行PCA，获取低维度降维主成分</p><p>或者我们传入一个浮点数，说明要保留的多少方差的主成分，让sklean来获取我们想要的的主成分个数</p><pre><code class="python">pca = PCA(n_components = 0.95)X_preduced = pca.fit_transform(X)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-K-means</title>
      <link href="undefined2019/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-K-means/"/>
      <url>2019/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-K-means/</url>
      
        <content type="html"><![CDATA[<h1 id="K-means-思路"><a href="#K-means-思路" class="headerlink" title="K-means 思路"></a>K-means 思路</h1><p>K-mean 是我接触到的第一个非监督学习算法，K-mean算法的思路想法是这样的，在我们获取到的所有Unmark点的空间中选取几个分类点，这些点的位置是我们随机产生的，我们把它们叫做聚类中心，我们将其分为两个类C1,C2叙述，我们打算把这批数据分类为两个分类，K-mean是一个迭代方法，他要做的是两个事情，1.簇分配；2.移动聚类中心。迭代所有数据点，计算数据点到两个聚类中心的距离，根据数据点到聚类中心的距离将其分配到C1,C2聚类中，簇分类的操作已经完成，还需要做的就是将聚类中心移动到刚才分配所得到的数据点中心去，具体做法就是计算单个聚类中所有数据点的均值，将聚类中心放置于均值点的位置。两个步骤完成，接下来就是反复循环上面的步骤再次执行检测分类。直到分类不再改变，最终就会找到两个簇</p><h1 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means 算法"></a>K-means 算法</h1><p>输入：</p><ul><li>K -&gt; 我们想要达到目的分为多少个类别</li><li>Train Set -&gt; 没有标签的数据集{x1,x2,x3,…xm}</li></ul><p>下列是借鉴的Andrew Ng老师机器学习的板书</p><pre><code class="python">Randomly initialize K cluster centroids u1,u2,u3...ukRepeat{    for i = 1 to m:        c(i) := 离x(i)点最近的聚类中心分类 min||x(i) - u(k)||^2    for k = 1 to K:        u(k) := 分配给集群簇的点的平均        # 例如：分配给C2的点为x(1),x(3),x(6),x(10)，计算u(2) = 1/4[x(1)+x(3)+x(6)+x(10)]}</code></pre><p>分类计算分配的点，如果发现有聚类中心u没有被分配点，可以选择删除聚类中心，但是会减少分类。</p><h1 id="K-mean-Optimization-Objective"><a href="#K-mean-Optimization-Objective" class="headerlink" title="K-mean Optimization Objective"></a>K-mean Optimization Objective</h1><p>K-mean 的优化目标最小化代价函数,最小化目标代价函数，最小化每个分类点到聚类中心的距离的平均</p><p><img src="kmeanOptimization.png" alt="kmeanoptimization"></p><h1 id="K-mean-Random-initialization"><a href="#K-mean-Random-initialization" class="headerlink" title="K-mean Random initialization"></a>K-mean Random initialization</h1><p>良好的初始化方法将会给模型带来一个好的分类，如何初始化聚合中心将会决定你获得到的结果好坏，不至于模型在数据集上坏掉</p><p>那么如何初始化模型的聚合中心，才能获得更好的模型，而且跳出局部最优</p><p>多次执行K-mean的方法</p><pre><code class="python">FOR i = 1 to 100{    Randomly initialize K-means.    Run K-means .Get c(1),...c(m),u(1),...u(K) 执行K-means 获得结果及聚类    计算成本函数Function(distortion)        J(c(1)...c(m),u1,...uK)}</code></pre><p>经过100次的计算我们获得了100中分类结果聚类数据，从中选取最优的，畸变值最低的cluster</p><h1 id="K-means-如何选择参数K"><a href="#K-means-如何选择参数K" class="headerlink" title="K-means 如何选择参数K"></a>K-means 如何选择参数K</h1><h2 id="肘部方法"><a href="#肘部方法" class="headerlink" title="肘部方法"></a>肘部方法</h2><p>通过从小改变K的值，对数据集做K-means，计算成本函数值。通过绘制K值与成本函数J的值的图像，观察成本函数下降趋势，转折肘部为佳选。</p><p><img src="elbow.png" alt="elbow"></p><p>肘部方法在某些成本函数下降平缓的情况下，无法作用</p><h2 id="从使用及商业角度考量"><a href="#从使用及商业角度考量" class="headerlink" title="从使用及商业角度考量"></a>从使用及商业角度考量</h2><p>从目的和商业角度考量K值应该取多少合适</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-Error Metrics for Skewed Classes</title>
      <link href="undefined2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Error-Metrics-for-Skewed-Classes/"/>
      <url>2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Error-Metrics-for-Skewed-Classes/</url>
      
        <content type="html"><![CDATA[<p>如果我们拥有的数据集是一批有极大偏差的数据集，比如癌症预测数据集，人类得癌症的几率是极低的，在这个数据集的测试集上我们获得了99%的正确率1%的错误率。</p><p>但我们查看了我们的数据集仅有很少量比如0.5%的人真正得了癌症。这就形成了很大的比例，在我们错误为1%的情况下，这个模型就显得并不是那么好，因此我们需要更好的判断误差的算法去评估我们模型的好坏。</p><h1 id="精度-precision-召回率-recall"><a href="#精度-precision-召回率-recall" class="headerlink" title="精度(precision)/ 召回率(recall)"></a>精度(precision)/ 召回率(recall)</h1><p>列区分的的是真实值的 分类 ，行区分的是预测值的 分类。比如True positive 是真实值为1 预测值也为1的 正确分类结果数，False positive 是真实值为1预测值为0的 错误分类结果数。</p><table><thead><tr><th>-</th><th>actual 1</th><th>actual 0</th></tr></thead><tbody><tr><td>predict 1</td><td>True positive</td><td>False positive</td></tr><tr><td>predict 0</td><td>False negative</td><td>True negative</td></tr></tbody></table><p>我们就有了新的一种方式来评估我们的模型</p><h2 id="精度-precision"><a href="#精度-precision" class="headerlink" title="精度(precision)"></a>精度(precision)</h2><p>解释：在我们模型预测为患有癌症的所有人当中，我们正确预测的比例</p><!-- $$ Precision = \frac{True positive}{predict positive} = \frac{True positive}{True positive + False positive} $$ --><p><img src="precision.png" alt="precision"></p><p>精度越高，模型就越好</p><h2 id="召回率-recall"><a href="#召回率-recall" class="headerlink" title="召回率(recall)"></a>召回率(recall)</h2><p>解释：在所有真正患癌症的人当中，我们正确预测到他们患癌症的比例</p><!-- $$ Recall = \frac{True positive}{Actual positive} = \frac{True positive}{True positive + False negative} $$ --><p><img src="recall.png" alt="recall"></p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1-Score"></a>F1-Score</h2><!-- $$ F1-Score = 2 x \frac{Precision x recall}{Precision + recall}$$ --><p><img src="f1.png" alt="F1-Score"></p><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p>不得不提之前使用的准确率，就是预测正确的数量在整个数据集上的比例</p><!-- $$Accuracy = \frac{True positive + False positive}{True positive + False positive + False negative + True negative}$$ --><p><img src="accuracy.png" alt="Accuracy"></p><p>在偏差较为大的情况，比如病情预测，我们使用F1-Score会能够更好的评估我们的模型</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-Cross Validation</title>
      <link href="undefined2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Cross-Validation/"/>
      <url>2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Cross-Validation/</url>
      
        <content type="html"><![CDATA[<p>当我们拿到一批数据，搭建了一个模型，准备为这个模型训练参数的时候，我们使用全部数据对模型计算损失和准确率，这会得到我们在整个训练集上的损失和准确率。这批数据是在我们训练过程中已经使用过的数据，那我们是如何判定这个模型在新的数据上的表现，是出色还是糟糕。</p><p>萌生这种想法，这种想法被称作交叉验证（Cross Validation）。我们拿到一批数据以后，并不是将这批数据全部都喂进我们的模型中，而是将这批数据做拆分，一批数据作为训练集，另外一批作为验证集。这样我们就能保证在验证集中获得的loss和acc是在未知的新数据中得到的。我们就可以以这个来评定我们的模型在新数据中表现的是出色还是糟糕。</p><h1 id="Shuffle-Data"><a href="#Shuffle-Data" class="headerlink" title="Shuffle Data"></a>Shuffle Data</h1><p>在拿到一批数据以后，有极大的可能这批数据集是有关联性的，比如是按照时间顺序排列，或者说前面部分是Class1 后面部分是Class2。这样在后面的数据集分离后，获得的数据集将会是不均很的，很可能在训练集当中70%都是Class1分类，而在验证集当中所有都是Class2分类。</p><p>为了避免这个问题的发生，我们的做法是，将数据集打乱，虽说不能完全避免这类情况的发生，但是极大降低了产生的几率。在本文中，仅将数据集切分划分为训练集和验证集，更权威的做法是将数据集分割成 训练集(train)、交叉验证集(cv)、测试集(test)，在训练集上训练我们的目标模型，在交叉验证集上找出我们较为好的模型（比如我们有多个多项式模型，需要寻找这个参数在几次方模型上表现的出色），最终在测试集上对刚才找出的模型做测试，得出在模型在新数据上的表现。具体操作做法在本文中不做讲解，可以参照下方的分割方法，大同小异</p><p>下面是我个人写的对数据集进行shuffle的代码，当然在现阶段的很多框架当中都有类似的方法可以直接拿来使用，比如sklearn中的shuffle。</p><pre><code class="python">import numpy as npp = np.random.permutation(x.shape[0])x = np.array(x)[p]y = np.array(y)[p]</code></pre><p>这样我们就获得了做了shuffle以后的data。就可以对其进行训练集验证集的划分。</p><h1 id="Simple交叉验证"><a href="#Simple交叉验证" class="headerlink" title="Simple交叉验证"></a>Simple交叉验证</h1><p>简单交叉验证就是单独将数据集做一次性划分，按照自己给定的比例将数据集做一个划分。通常我们将70%的数据划分作为训练集，30%的数据划分作为验证集。 训练集 &gt; 验证集。当然 Up to you.</p><pre><code class="python">if rate &gt;= 1 or rate &lt;=0 :    raise Exception(&quot;请设置rate为(0,1)之间的数&quot;)train_ind = int(x.shape[0] * rate)np.array(x[:train_ind]),np.array(x[train_ind+1:]),np.array(y[:train_ind]),np.array(y[train_ind+1:])</code></pre><p>所实现的更能和sklearn中的拆分类似，下面展示我的所有代码。</p><pre><code class="python">def shuffle_split(x, y , rate = 0.7):    &quot;&quot;&quot;    对数据进行shuffle，切分数据集为训练集和验证集    &quot;&quot;&quot;    if rate &gt;= 1 or rate &lt;=0 :        raise Exception(&quot;请设置rate为(0,1)之间的数&quot;)    p = np.random.permutation(x.shape[0])    x = np.array(x)[p]    y = np.array(y)[p]    train_ind = int(x.shape[0] * rate)    return np.array(x[:train_ind]),np.array(x[train_ind+1:]),np.array(y[:train_ind]),np.array(y[train_ind+1:])</code></pre><h1 id="K-Fold-交叉验证"><a href="#K-Fold-交叉验证" class="headerlink" title="K-Fold 交叉验证"></a>K-Fold 交叉验证</h1><p>K-Fild 数据集拆分K份，在每轮训练中，将使用不同的训练集：将随机使用K-1份作为训练集，单一的那份作为验证集，当该轮训练结束后需要重新获取选择新的K-1份数据集作为训练集。</p><p>代码实现自己造轮子吧。当然也可使用各框架中的方法。Up to you</p><p>Best wishes</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络-Random Initialization</title>
      <link href="undefined2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Random-Initialization/"/>
      <url>2019/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Random-Initialization/</url>
      
        <content type="html"><![CDATA[<p>在机器学习的过程中参数必然需要进行初始化操作，我们具体把参数初始化为什么较好呢？是否可以将所有的权重向量设置初始化为同样的值，比如0？</p><p>答案是否定的，在吴恩达老师讲解的机器学习课程中，对于这一个初始化做了详细的讲解。</p><h1 id="Zero-initialization"><a href="#Zero-initialization" class="headerlink" title="Zero initialization"></a>Zero initialization</h1><p>将所有参数在初始化的时候初始化为0，我们可以试着这么理解，在前端输入层输入了相同的数据，我们根据这个均相同的参数权重去计算下一层的输入。这就意味这我们会以相同的输入函数去计算，对每个输入的sample去训练，这样来说我们下一层的所有输入将会是相同的，这样一来，我们在后面获得的损失函数对前面参数的求导，获得的梯度也会是相同的，根据学习率向着相同的方向做梯度更新以后，获得的参数也将会是相同的。最后在更新以后每层隐含层的参数是相同的，这就意味着整个神经网络参数高度冗余，在最后的输出单元只会得到单个功能。因此zero initialization 在神经网络中是不可行的</p><p>因此我们需要更有特色，更有用的参数权重初始化方法，使得我们的学习是有用的学习，在模型中能有更多的功能，或者这么说，我们可以获得一个更好的，较少冗余的神经网络模型。</p><h1 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h1><p>Random Initialization 是神经网络初始化的一种方法。</p><p>随机初始化的方法，给定初始化为 rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON</p><p>此处的INIT_EPSILON 是一个非常接近于0 的初始值,很好的一个选择方法是如下的公式</p><!-- $$\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}} $$ --><p><img src="ramdominitialization.png" alt="ramdominitialization"></p><p>此处的<!-- $L_{in} = s_l and L_{out} = s_{l+1}$ --> <img src="random1.png" alt="ramdom1">为初始化参数θ相应网络层中的神经元个数</p><p>通过这个方法初始化所有的参数，可以打破对称。从而跳出之前在使用Zero initialization 时候遇到的问题。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MachineLearning with SK AND TF (1_端到端的机器学习项目)</title>
      <link href="undefined2019/08/23/MachineLearning-with-SK-AND-TF-1-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
      <url>2019/08/23/MachineLearning-with-SK-AND-TF-1-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<pre><code class="python"># download datasetimport osimport tarfileimport urllibDOWNLOAD_ROOT = &quot;http://raw.githubusercontent.com/ageron/handson-ml/master/&quot;HOUSING_PATH = &quot;datasets/housing&quot;HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + &quot;/housing.tgz&quot;print(HOUSING_URL)def fetch_housing_data(housing_url = HOUSING_URL,housing_path = HOUSING_PATH):    if not os.path.isdir(housing_path):        os.makedirs(housing_path)    tgz_path = os.path.join(housing_path,&quot;housing.tgz&quot;)    urllib.request.urlretrieve(housing_url,tgz_path)    housing_tgz = tarfile.open(tgz_path)    housing_tgz.extractall(path=housing_path)    housing_tgz.close()</code></pre><pre><code>http://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz</code></pre><pre><code class="python"># load dataimport pandas as pddef load_housing_data(housing_path = HOUSING_PATH):    csv_path = os.path.join(housing_path, &quot;housing.csv&quot;)    return pd.read_csv(csv_path)</code></pre><pre><code class="python"># 调用if not os.path.isfile(HOUSING_PATH):    fetch_housing_data()housing = load_housing_data()housing</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>      <th>ocean_proximity</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-122.23</td>      <td>37.88</td>      <td>41.0</td>      <td>880.0</td>      <td>129.0</td>      <td>322.0</td>      <td>126.0</td>      <td>8.3252</td>      <td>452600.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>1</th>      <td>-122.22</td>      <td>37.86</td>      <td>21.0</td>      <td>7099.0</td>      <td>1106.0</td>      <td>2401.0</td>      <td>1138.0</td>      <td>8.3014</td>      <td>358500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>2</th>      <td>-122.24</td>      <td>37.85</td>      <td>52.0</td>      <td>1467.0</td>      <td>190.0</td>      <td>496.0</td>      <td>177.0</td>      <td>7.2574</td>      <td>352100.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>3</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1274.0</td>      <td>235.0</td>      <td>558.0</td>      <td>219.0</td>      <td>5.6431</td>      <td>341300.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>4</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1627.0</td>      <td>280.0</td>      <td>565.0</td>      <td>259.0</td>      <td>3.8462</td>      <td>342200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>5</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>919.0</td>      <td>213.0</td>      <td>413.0</td>      <td>193.0</td>      <td>4.0368</td>      <td>269700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>6</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>2535.0</td>      <td>489.0</td>      <td>1094.0</td>      <td>514.0</td>      <td>3.6591</td>      <td>299200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>7</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>3104.0</td>      <td>687.0</td>      <td>1157.0</td>      <td>647.0</td>      <td>3.1200</td>      <td>241400.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>8</th>      <td>-122.26</td>      <td>37.84</td>      <td>42.0</td>      <td>2555.0</td>      <td>665.0</td>      <td>1206.0</td>      <td>595.0</td>      <td>2.0804</td>      <td>226700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>9</th>      <td>-122.25</td>      <td>37.84</td>      <td>52.0</td>      <td>3549.0</td>      <td>707.0</td>      <td>1551.0</td>      <td>714.0</td>      <td>3.6912</td>      <td>261100.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>10</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2202.0</td>      <td>434.0</td>      <td>910.0</td>      <td>402.0</td>      <td>3.2031</td>      <td>281500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>11</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>3503.0</td>      <td>752.0</td>      <td>1504.0</td>      <td>734.0</td>      <td>3.2705</td>      <td>241800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>12</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2491.0</td>      <td>474.0</td>      <td>1098.0</td>      <td>468.0</td>      <td>3.0750</td>      <td>213500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>13</th>      <td>-122.26</td>      <td>37.84</td>      <td>52.0</td>      <td>696.0</td>      <td>191.0</td>      <td>345.0</td>      <td>174.0</td>      <td>2.6736</td>      <td>191300.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>14</th>      <td>-122.26</td>      <td>37.85</td>      <td>52.0</td>      <td>2643.0</td>      <td>626.0</td>      <td>1212.0</td>      <td>620.0</td>      <td>1.9167</td>      <td>159200.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>15</th>      <td>-122.26</td>      <td>37.85</td>      <td>50.0</td>      <td>1120.0</td>      <td>283.0</td>      <td>697.0</td>      <td>264.0</td>      <td>2.1250</td>      <td>140000.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>16</th>      <td>-122.27</td>      <td>37.85</td>      <td>52.0</td>      <td>1966.0</td>      <td>347.0</td>      <td>793.0</td>      <td>331.0</td>      <td>2.7750</td>      <td>152500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>17</th>      <td>-122.27</td>      <td>37.85</td>      <td>52.0</td>      <td>1228.0</td>      <td>293.0</td>      <td>648.0</td>      <td>303.0</td>      <td>2.1202</td>      <td>155500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>18</th>      <td>-122.26</td>      <td>37.84</td>      <td>50.0</td>      <td>2239.0</td>      <td>455.0</td>      <td>990.0</td>      <td>419.0</td>      <td>1.9911</td>      <td>158700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>19</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>1503.0</td>      <td>298.0</td>      <td>690.0</td>      <td>275.0</td>      <td>2.6033</td>      <td>162900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>20</th>      <td>-122.27</td>      <td>37.85</td>      <td>40.0</td>      <td>751.0</td>      <td>184.0</td>      <td>409.0</td>      <td>166.0</td>      <td>1.3578</td>      <td>147500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>21</th>      <td>-122.27</td>      <td>37.85</td>      <td>42.0</td>      <td>1639.0</td>      <td>367.0</td>      <td>929.0</td>      <td>366.0</td>      <td>1.7135</td>      <td>159800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>22</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>2436.0</td>      <td>541.0</td>      <td>1015.0</td>      <td>478.0</td>      <td>1.7250</td>      <td>113900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>23</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>1688.0</td>      <td>337.0</td>      <td>853.0</td>      <td>325.0</td>      <td>2.1806</td>      <td>99700.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>24</th>      <td>-122.27</td>      <td>37.84</td>      <td>52.0</td>      <td>2224.0</td>      <td>437.0</td>      <td>1006.0</td>      <td>422.0</td>      <td>2.6000</td>      <td>132600.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>25</th>      <td>-122.28</td>      <td>37.85</td>      <td>41.0</td>      <td>535.0</td>      <td>123.0</td>      <td>317.0</td>      <td>119.0</td>      <td>2.4038</td>      <td>107500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>26</th>      <td>-122.28</td>      <td>37.85</td>      <td>49.0</td>      <td>1130.0</td>      <td>244.0</td>      <td>607.0</td>      <td>239.0</td>      <td>2.4597</td>      <td>93800.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>27</th>      <td>-122.28</td>      <td>37.85</td>      <td>52.0</td>      <td>1898.0</td>      <td>421.0</td>      <td>1102.0</td>      <td>397.0</td>      <td>1.8080</td>      <td>105500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>28</th>      <td>-122.28</td>      <td>37.84</td>      <td>50.0</td>      <td>2082.0</td>      <td>492.0</td>      <td>1131.0</td>      <td>473.0</td>      <td>1.6424</td>      <td>108900.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>29</th>      <td>-122.28</td>      <td>37.84</td>      <td>52.0</td>      <td>729.0</td>      <td>160.0</td>      <td>395.0</td>      <td>155.0</td>      <td>1.6875</td>      <td>132000.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>20610</th>      <td>-121.56</td>      <td>39.10</td>      <td>28.0</td>      <td>2130.0</td>      <td>484.0</td>      <td>1195.0</td>      <td>439.0</td>      <td>1.3631</td>      <td>45500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20611</th>      <td>-121.55</td>      <td>39.10</td>      <td>27.0</td>      <td>1783.0</td>      <td>441.0</td>      <td>1163.0</td>      <td>409.0</td>      <td>1.2857</td>      <td>47000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20612</th>      <td>-121.56</td>      <td>39.08</td>      <td>26.0</td>      <td>1377.0</td>      <td>289.0</td>      <td>761.0</td>      <td>267.0</td>      <td>1.4934</td>      <td>48300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20613</th>      <td>-121.55</td>      <td>39.09</td>      <td>31.0</td>      <td>1728.0</td>      <td>365.0</td>      <td>1167.0</td>      <td>384.0</td>      <td>1.4958</td>      <td>53400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20614</th>      <td>-121.54</td>      <td>39.08</td>      <td>26.0</td>      <td>2276.0</td>      <td>460.0</td>      <td>1455.0</td>      <td>474.0</td>      <td>2.4695</td>      <td>58000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20615</th>      <td>-121.54</td>      <td>39.08</td>      <td>23.0</td>      <td>1076.0</td>      <td>216.0</td>      <td>724.0</td>      <td>197.0</td>      <td>2.3598</td>      <td>57500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20616</th>      <td>-121.53</td>      <td>39.08</td>      <td>15.0</td>      <td>1810.0</td>      <td>441.0</td>      <td>1157.0</td>      <td>375.0</td>      <td>2.0469</td>      <td>55100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20617</th>      <td>-121.53</td>      <td>39.06</td>      <td>20.0</td>      <td>561.0</td>      <td>109.0</td>      <td>308.0</td>      <td>114.0</td>      <td>3.3021</td>      <td>70800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20618</th>      <td>-121.55</td>      <td>39.06</td>      <td>25.0</td>      <td>1332.0</td>      <td>247.0</td>      <td>726.0</td>      <td>226.0</td>      <td>2.2500</td>      <td>63400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20619</th>      <td>-121.56</td>      <td>39.01</td>      <td>22.0</td>      <td>1891.0</td>      <td>340.0</td>      <td>1023.0</td>      <td>296.0</td>      <td>2.7303</td>      <td>99100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20620</th>      <td>-121.48</td>      <td>39.05</td>      <td>40.0</td>      <td>198.0</td>      <td>41.0</td>      <td>151.0</td>      <td>48.0</td>      <td>4.5625</td>      <td>100000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20621</th>      <td>-121.47</td>      <td>39.01</td>      <td>37.0</td>      <td>1244.0</td>      <td>247.0</td>      <td>484.0</td>      <td>157.0</td>      <td>2.3661</td>      <td>77500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20622</th>      <td>-121.44</td>      <td>39.00</td>      <td>20.0</td>      <td>755.0</td>      <td>147.0</td>      <td>457.0</td>      <td>157.0</td>      <td>2.4167</td>      <td>67000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20623</th>      <td>-121.37</td>      <td>39.03</td>      <td>32.0</td>      <td>1158.0</td>      <td>244.0</td>      <td>598.0</td>      <td>227.0</td>      <td>2.8235</td>      <td>65500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20624</th>      <td>-121.41</td>      <td>39.04</td>      <td>16.0</td>      <td>1698.0</td>      <td>300.0</td>      <td>731.0</td>      <td>291.0</td>      <td>3.0739</td>      <td>87200.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20625</th>      <td>-121.52</td>      <td>39.12</td>      <td>37.0</td>      <td>102.0</td>      <td>17.0</td>      <td>29.0</td>      <td>14.0</td>      <td>4.1250</td>      <td>72000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20626</th>      <td>-121.43</td>      <td>39.18</td>      <td>36.0</td>      <td>1124.0</td>      <td>184.0</td>      <td>504.0</td>      <td>171.0</td>      <td>2.1667</td>      <td>93800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20627</th>      <td>-121.32</td>      <td>39.13</td>      <td>5.0</td>      <td>358.0</td>      <td>65.0</td>      <td>169.0</td>      <td>59.0</td>      <td>3.0000</td>      <td>162500.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20628</th>      <td>-121.48</td>      <td>39.10</td>      <td>19.0</td>      <td>2043.0</td>      <td>421.0</td>      <td>1018.0</td>      <td>390.0</td>      <td>2.5952</td>      <td>92400.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20629</th>      <td>-121.39</td>      <td>39.12</td>      <td>28.0</td>      <td>10035.0</td>      <td>1856.0</td>      <td>6912.0</td>      <td>1818.0</td>      <td>2.0943</td>      <td>108300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20630</th>      <td>-121.32</td>      <td>39.29</td>      <td>11.0</td>      <td>2640.0</td>      <td>505.0</td>      <td>1257.0</td>      <td>445.0</td>      <td>3.5673</td>      <td>112000.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20631</th>      <td>-121.40</td>      <td>39.33</td>      <td>15.0</td>      <td>2655.0</td>      <td>493.0</td>      <td>1200.0</td>      <td>432.0</td>      <td>3.5179</td>      <td>107200.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20632</th>      <td>-121.45</td>      <td>39.26</td>      <td>15.0</td>      <td>2319.0</td>      <td>416.0</td>      <td>1047.0</td>      <td>385.0</td>      <td>3.1250</td>      <td>115600.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20633</th>      <td>-121.53</td>      <td>39.19</td>      <td>27.0</td>      <td>2080.0</td>      <td>412.0</td>      <td>1082.0</td>      <td>382.0</td>      <td>2.5495</td>      <td>98300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20634</th>      <td>-121.56</td>      <td>39.27</td>      <td>28.0</td>      <td>2332.0</td>      <td>395.0</td>      <td>1041.0</td>      <td>344.0</td>      <td>3.7125</td>      <td>116800.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20635</th>      <td>-121.09</td>      <td>39.48</td>      <td>25.0</td>      <td>1665.0</td>      <td>374.0</td>      <td>845.0</td>      <td>330.0</td>      <td>1.5603</td>      <td>78100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20636</th>      <td>-121.21</td>      <td>39.49</td>      <td>18.0</td>      <td>697.0</td>      <td>150.0</td>      <td>356.0</td>      <td>114.0</td>      <td>2.5568</td>      <td>77100.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20637</th>      <td>-121.22</td>      <td>39.43</td>      <td>17.0</td>      <td>2254.0</td>      <td>485.0</td>      <td>1007.0</td>      <td>433.0</td>      <td>1.7000</td>      <td>92300.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20638</th>      <td>-121.32</td>      <td>39.43</td>      <td>18.0</td>      <td>1860.0</td>      <td>409.0</td>      <td>741.0</td>      <td>349.0</td>      <td>1.8672</td>      <td>84700.0</td>      <td>INLAND</td>    </tr>    <tr>      <th>20639</th>      <td>-121.24</td>      <td>39.37</td>      <td>16.0</td>      <td>2785.0</td>      <td>616.0</td>      <td>1387.0</td>      <td>530.0</td>      <td>2.3886</td>      <td>89400.0</td>      <td>INLAND</td>    </tr>  </tbody></table><p>20640 rows × 10 columns</p></div><pre><code class="python">housing.info()</code></pre><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 20640 entries, 0 to 20639Data columns (total 10 columns):longitude             20640 non-null float64latitude              20640 non-null float64housing_median_age    20640 non-null float64total_rooms           20640 non-null float64total_bedrooms        20433 non-null float64population            20640 non-null float64households            20640 non-null float64median_income         20640 non-null float64median_house_value    20640 non-null float64ocean_proximity       20640 non-null objectdtypes: float64(9), object(1)memory usage: 1.6+ MB</code></pre><pre><code class="python"># 查看数据集housing.describe()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20433.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>    </tr>    <tr>      <th>mean</th>      <td>-119.569704</td>      <td>35.631861</td>      <td>28.639486</td>      <td>2635.763081</td>      <td>537.870553</td>      <td>1425.476744</td>      <td>499.539680</td>      <td>3.870671</td>      <td>206855.816909</td>    </tr>    <tr>      <th>std</th>      <td>2.003532</td>      <td>2.135952</td>      <td>12.585558</td>      <td>2181.615252</td>      <td>421.385070</td>      <td>1132.462122</td>      <td>382.329753</td>      <td>1.899822</td>      <td>115395.615874</td>    </tr>    <tr>      <th>min</th>      <td>-124.350000</td>      <td>32.540000</td>      <td>1.000000</td>      <td>2.000000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>1.000000</td>      <td>0.499900</td>      <td>14999.000000</td>    </tr>    <tr>      <th>25%</th>      <td>-121.800000</td>      <td>33.930000</td>      <td>18.000000</td>      <td>1447.750000</td>      <td>296.000000</td>      <td>787.000000</td>      <td>280.000000</td>      <td>2.563400</td>      <td>119600.000000</td>    </tr>    <tr>      <th>50%</th>      <td>-118.490000</td>      <td>34.260000</td>      <td>29.000000</td>      <td>2127.000000</td>      <td>435.000000</td>      <td>1166.000000</td>      <td>409.000000</td>      <td>3.534800</td>      <td>179700.000000</td>    </tr>    <tr>      <th>75%</th>      <td>-118.010000</td>      <td>37.710000</td>      <td>37.000000</td>      <td>3148.000000</td>      <td>647.000000</td>      <td>1725.000000</td>      <td>605.000000</td>      <td>4.743250</td>      <td>264725.000000</td>    </tr>    <tr>      <th>max</th>      <td>-114.310000</td>      <td>41.950000</td>      <td>52.000000</td>      <td>39320.000000</td>      <td>6445.000000</td>      <td>35682.000000</td>      <td>6082.000000</td>      <td>15.000100</td>      <td>500001.000000</td>    </tr>  </tbody></table></div><pre><code class="python">%matplotlib inlineimport matplotlib.pyplot as plthousing.hist(bins=50, figsize=(20,15))plt.show()</code></pre><p><img src="output_5_0.png" alt="png"></p><pre><code class="python"># Prpare data## Train Test splitfrom sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(housing,test_size=0.2 , random_state = 42)</code></pre><pre><code class="python"># 上面的方法的缺陷，对于某个数据集，我们不能保证训练集和测试集都有这个字段的每个类别，因此我们需要按照下列方法使用分层抽样的方法对训练测试数据集进行分割割## 分层抽样 对income_cat进行抽样import numpy as np### 根据收入分层，变成几个类别，数据集乘1.5限制收入类别数量，使用ceil对数据集进行取整，得到离散类别housing[&quot;income_cat&quot;] = np.ceil(housing[&quot;median_income&quot;] / 1.5)housing[&quot;income_cat&quot;].where(housing[&quot;income_cat&quot;] &lt;5 ,5.0, inplace = True) # where 替换大于等于5.0的数据为5.0# 对incomg收入进行分类以后，进行分层抽样from sklearn.model_selection import StratifiedShuffleSplit # 分层洗牌拆分split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state = 42)for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): # 在数据集中使用income_cat 对housing进行分层抽样    strat_train_set = housing.loc[train_index]    strat_test_set = housing.loc[test_index]# 查看数据集中每个分层所占比例housing[&quot;income_cat&quot;].value_counts() / len(housing)</code></pre><pre><code>3.0    0.3505812.0    0.3188474.0    0.1763085.0    0.1144381.0    0.039826Name: income_cat, dtype: float64</code></pre><pre><code class="python"># drop income_catfor setdt in (strat_train_set,strat_test_set):    setdt.drop([&#39;income_cat&#39;],axis = 1 , inplace = True)</code></pre><pre><code class="python">housing = strat_train_set.copy()housing.plot(kind = &quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;,alpha = 0.4,            s = housing[&quot;population&quot;]/100,label = &quot;population&quot;,            c = &quot;median_house_value&quot;,cmap = plt.get_cmap(&quot;jet&quot;),colorbar = True)plt.legend()</code></pre><pre><code>&lt;matplotlib.legend.Legend at 0x142354e0&gt;</code></pre><p><img src="output_9_1.png" alt="png"></p><pre><code class="python"># 数据集不大，可以使用corr()函数计算没对属性之间“标准相关系数（皮尔逊相关系数）”corr_metrix = housing.corr()corr_metrix[&quot;median_house_value&quot;].sort_values(ascending = False)</code></pre><pre><code>median_house_value    1.000000median_income         0.687160total_rooms           0.135097housing_median_age    0.114110households            0.064506total_bedrooms        0.047689population           -0.026920longitude            -0.047432latitude             -0.142724Name: median_house_value, dtype: float64</code></pre><pre><code class="python">from pandas.plotting import scatter_matrix# scatter_matrix 绘制出每个数值属性相对于其他数值属性的相关性，在此我们有11个数值属性，可以得到11**2 = 121图像，我们可以选取相应的数值展示attributes = [&quot;median_house_value&quot;,&quot;median_income&quot;,&quot;total_rooms&quot;,&quot;housing_median_age&quot;]scatter_matrix(housing[attributes],figsize = (12,8))# 在对角线上原本为自身对照，这没有意义，因此，pandas取而代之的是每个属性的直方图</code></pre><pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001495A630&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001470D048&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000147326D8&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014758D68&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001478B438&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001478B470&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014AB5160&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014ADE7F0&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B07E80&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B38550&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B5FBE0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014B922B0&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014DAA940&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014DD1FD0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000014E036A0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000001504CD30&gt;]],      dtype=object)</code></pre><p><img src="output_11_1.png" alt="png"></p><pre><code class="python"># 尝试根据现有数据集创建新的觉得有用的数据housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;] / housing[&quot;households&quot;]housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;] / housing[&quot;total_rooms&quot;]housing[&quot;population_per_household&quot;] = housing[&quot;population&quot;] / housing[&quot;households&quot;]</code></pre><hr><pre><code class="python"># 重新获取一组新数据我们用来处理housing = strat_train_set.drop(&quot;median_house_value&quot;,axis = 1)housing_labels = strat_train_set[&quot;median_house_value&quot;].copy()housing.info()housing.shape</code></pre><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;Int64Index: 16512 entries, 17606 to 15775Data columns (total 9 columns):longitude             16512 non-null float64latitude              16512 non-null float64housing_median_age    16512 non-null float64total_rooms           16512 non-null float64total_bedrooms        16354 non-null float64population            16512 non-null float64households            16512 non-null float64median_income         16512 non-null float64ocean_proximity       16512 non-null objectdtypes: float64(8), object(1)memory usage: 1.3+ MB(16512, 9)</code></pre><pre><code class="python"># 处理数值属性## 对于属性缺失的问题，如上面的total_badrooms属性:# housing.dropna(subset=[&quot;total_bedrooms&quot;])### 放弃相应地区# housing.drop(&quot;total_bedrooms&quot;,axis =1)### 放弃这个属性# median = housing[&quot;total_bedrooms&quot;].median()# housing[&quot;total_bedrooms&quot;].fillna(median)### 将缺失属性设置为某个默认值（0，平均数，中位数）from sklearn.preprocessing import Imputer# 不确定其他属性值是否会存在缺失值，就把inputer应用于所有属性imputer = Imputer(strategy = &quot;median&quot;)### 使用sklearn中的api可以轻松实现第三种housing_num = housing.drop(&quot;ocean_proximity&quot;,axis = 1) # 创建临时数据集，该数据集没有文本属性imputer.fit(housing_num) # inputer将处理后的中位数存放于statistics_中X = imputer.transform(housing_num) # 回传numpy数组housing_tr = pd.DataFrame(X,columns=housing_num.columns) # 转为pandas</code></pre><pre><code class="python"># 处理文本和分类属性housing_cat = housing[&quot;ocean_proximity&quot;]&quot;&quot;&quot;## sklearn提供LabelEncoderfrom sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()housing_cat_encoded = encoder.fit_transform(housing_cat)housing_cat_encoded## 将label转为onehot编码from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))housing_cat_1hot.toarray()&quot;&quot;&quot;# LabelBinarizer from sklearn.preprocessing import LabelBinarizerencoder = LabelBinarizer()housing_cat_1hot = encoder.fit_transform(housing_cat)# 我们可以自定义转换器from sklearn.base import BaseEstimator,TransformerMixinrooms_ix, bedrooms_ix,population_ix, household_ix = 3,4,5,6class CombinedAttributesAdder(BaseEstimator,TransformerMixin):    def __init__(self, add_bedrooms_per_rooms = True):        self.add_bedrooms_per_rooms = add_bedrooms_per_rooms    def fit(self,X,y = None):        return self    def transform(self,X,y = None):        rooms_per_household = X[:,rooms_ix] / X[:,household_ix]        population_per_household = X[:,population_ix] / X[:,household_ix]        if self.add_bedrooms_per_rooms:            bedrooms_per_rooms = X[:,bedrooms_ix] / X[:,rooms_ix]            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_rooms]        else:            return np.c_[X,rooms_per_household,population_per_household]sttr_adder = CombinedAttributesAdder(add_bedrooms_per_rooms = False)housing_extra_attribs = sttr_adder.transform(housing.values)</code></pre><pre><code class="python">## 使用pipeline创建流水线转换处理数据from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScaler # 数据标准化num_pipeline = Pipeline([    (&#39;imputer&#39;, Imputer(strategy=&#39;median&#39;)),    (&#39;attribs_adder&#39;, CombinedAttributesAdder()),    (&#39;std_scaler&#39;,StandardScaler())])# 每个转换器都需要有fit_transform方法，调用fit函数则会按照顺序执行fit_transform(),如果不希望调用完fit()后再调用transform，可以直接调用fit_transformhousing_num_tr = num_pipeline.fit_transform(housing_num)class DataFrameSelector(BaseEstimator,TransformerMixin):    def __init__(self,attribute_names):        self.attribute_names = attribute_names    def fit(self,X,y = None):        return self    def transform(self,X,y = None):        return X[self.attribute_names].values# 由于LabelBinarizer默认传参为2，后面调用会报错，可以定义一个自己的LabelBinarizer类class MyLabelBinarizer(TransformerMixin):    def __init__(self,*args, **kwargs):        self.encoder = LabelBinarizer(*args, **kwargs)    def fit(self,X,y = None):        self.encoder.fit(X)        return self    def transform(self,X,y = None):        return self.encoder.transform(X)# 如果有多个pipeline，我们可以使用如下方法进行拼接from sklearn.pipeline import FeatureUnionnum_attribs = list(housing_num)cat_attribs = [&#39;ocean_proximity&#39;]num_pipeline = Pipeline([    (&#39;selector&#39;, DataFrameSelector(num_attribs)),    (&#39;imputer&#39;, Imputer(strategy=&#39;median&#39;)),    (&#39;attribs_adder&#39;, CombinedAttributesAdder()),    (&#39;std_scaler&#39;,StandardScaler())])cat_pipeline = Pipeline([    (&#39;selector&#39;,DataFrameSelector(cat_attribs)),    (&#39;label_binarizer&#39;, MyLabelBinarizer()),])full_pipeline = FeatureUnion(transformer_list = [    (&quot;num_pipeline&quot;,num_pipeline),    (&quot;cat_pipeline&quot;,cat_pipeline),])housing_prepared = full_pipeline.fit_transform(housing)print(housing_prepared)print(housing_prepared.shape)print(housing_prepared[0])</code></pre><pre><code>[[-1.15604281  0.77194962  0.74333089 ...  0.          0.   0.        ] [-1.17602483  0.6596948  -1.1653172  ...  0.          0.   0.        ] [ 1.18684903 -1.34218285  0.18664186 ...  0.          0.   1.        ] ... [ 1.58648943 -0.72478134 -1.56295222 ...  0.          0.   0.        ] [ 0.78221312 -0.85106801  0.18664186 ...  0.          0.   0.        ] [-1.43579109  0.99645926  1.85670895 ...  0.          1.   0.        ]](16512, 16)[-1.15604281  0.77194962  0.74333089 -0.49323393 -0.44543821 -0.63621141 -0.42069842 -0.61493744 -0.31205452 -0.08649871  0.15531753  1.  0.          0.          0.          0.        ]</code></pre><hr><h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><pre><code class="python"># 线性回归模型的训练from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</code></pre><pre><code class="python"># 使用几个训练集的实例测试我们的线性回归模型some_data = housing.iloc[:5]some_datasome_labels = housing_labels.iloc[:5]some_data_prepared = full_pipeline.transform(some_data)print(&quot;Predictions: \t&quot;, lin_reg.predict(some_data_prepared))print(&quot;Labels:\t\t&quot;,list(some_labels)) # 虽然可以工作了预测准确度太差# 可以使用sklearn中RMSE(mean-squared-error)测量训练集上回归模型的RMSEfrom sklearn.metrics import mean_squared_errorhousing_predictions = lin_reg.predict(housing_prepared)lin_mse = mean_squared_error(housing_labels,housing_predictions)print(&quot;Train-MSE:%s&quot;%lin_mse)lin_rmse = np.sqrt(lin_mse)print(&quot;Train-RMSE：%s&quot;%lin_rmse)</code></pre><pre><code>Predictions:      [210644.60459286 317768.80697211 210956.43331178  59218.98886849 189747.55849879]Labels:         [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]Train-MSE:4709829587.971121Train-RMSE：68628.19819848923</code></pre><pre><code class="python"># 使用sklearn中的交叉验证功能# 降训练集随机分成10份，每个子集称为一个折叠fold，对决策树模型进行十次训练和评估，每次调训一个折叠作为评估，另外9份作为训练集。from sklearn.model_selection import cross_val_scorelin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=&quot;neg_mean_squared_error&quot;,cv = 10)lin_rmse_scores = np.sqrt(-lin_scores)print(&#39;Scores:&#39;,lin_rmse_scores)print(&#39;Mean（均值）:&#39;,lin_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,lin_rmse_scores.std())</code></pre><pre><code>Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067]Mean（均值）: 69052.46136345083Standard deviation(标准偏差): 2731.6740017983425</code></pre><hr><h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><pre><code class="python">from sklearn.tree import DecisionTreeClassifiertree_reg = DecisionTreeClassifier()tree_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,            max_features=None, max_leaf_nodes=None,            min_impurity_decrease=0.0, min_impurity_split=None,            min_samples_leaf=1, min_samples_split=2,            min_weight_fraction_leaf=0.0, presort=False, random_state=None,            splitter=&#39;best&#39;)</code></pre><pre><code class="python">housing_predictions = tree_reg.predict(housing_prepared)tree_mse = mean_squared_error(housing_labels,housing_predictions)tree_rmse = np.sqrt(tree_mse)tree_rmse #已过拟合，我们可以使用之前分割的测试数据集进行验证或者使用交叉验证</code></pre><pre><code>0.0</code></pre><pre><code class="python">tree_scores = cross_val_score(tree_reg,housing_prepared,housing_labels,scoring = &#39;neg_mean_squared_error&#39;,cv = 10)tree_rmse_scores = np.sqrt(-tree_scores)</code></pre><pre><code>d:\ipython\lib\site-packages\sklearn\model_selection\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.  % (min_groups, self.n_splits)), Warning)</code></pre><pre><code class="python">print(&#39;Scores:&#39;,tree_rmse_scores)print(&#39;Mean（均值）:&#39;,tree_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,tree_rmse_scores.std())</code></pre><pre><code>Scores: [ 85710.01369961  81230.61378336  82335.83537005  75877.27486535  75090.99888798  78409.26002322  78475.89222942  79428.78633416 103606.00563702 101828.54775259]Mean（均值）: 84199.322858276Standard deviation(标准偏差): 9712.031022021029</code></pre><hr><h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><pre><code class="python">from sklearn.ensemble import RandomForestRegressorforest_reg = RandomForestRegressor()forest_reg.fit(housing_prepared,housing_labels)</code></pre><pre><code>RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=&#39;auto&#39;, max_leaf_nodes=None,           min_impurity_decrease=0.0, min_impurity_split=None,           min_samples_leaf=1, min_samples_split=2,           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,           oob_score=False, random_state=None, verbose=0, warm_start=False)</code></pre><pre><code class="python">housing_predictions = forest_reg.predict(housing_prepared)forest_mse = mean_squared_error(housing_labels,housing_predictions)forest_rmse = np.sqrt(forest_mse)forest_rmse</code></pre><pre><code>22174.812789705677</code></pre><pre><code class="python">forest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring = &#39;neg_mean_squared_error&#39;,cv = 10)forest_rmse_scores = np.sqrt(-forest_scores)print(&#39;Scores:&#39;,forest_rmse_scores)print(&#39;Mean（均值）:&#39;,forest_rmse_scores.mean())print(&#39;Standard deviation(标准偏差):&#39;,forest_rmse_scores.std())</code></pre><pre><code>Scores: [52673.69656378 50803.5706211  53741.47818017 53343.47330547 51028.26110671 56342.5218843  50757.88436481 50711.95588612 55791.4729642  52200.01381263]Mean（均值）: 52739.43286892828Standard deviation(标准偏差): 1966.5788024649446</code></pre><hr><h1 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h1><p>上面的模型我们知识大概了解一下。现在我们有几组比较有效的模型候选列表，我们可以使用网格搜索的方式去调整超参数</p><p>下面我们使用GridSearchCV对RandomForestRegressor进行网格搜索调整参数</p><pre><code class="python">from sklearn.model_selection import GridSearchCVparam_guid = [    {&#39;n_estimators&#39;:[3,10,30],&#39;max_features&#39;:[2,4,6,8]},    {&#39;bootstrap&#39;:[False],&#39;n_estimators&#39;:[3,10],&#39;max_features&#39;:[2,3,4]},]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg,param_guid,cv = 5,scoring = &#39;neg_mean_squared_error&#39;)grid_search.fit(housing_prepared,housing_labels)</code></pre><pre><code>GridSearchCV(cv=5, error_score=&#39;raise&#39;,       estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=&#39;auto&#39;, max_leaf_nodes=None,           min_impurity_decrease=0.0, min_impurity_split=None,           min_samples_leaf=1, min_samples_split=2,           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,           oob_score=False, random_state=None, verbose=0, warm_start=False),       fit_params=None, iid=True, n_jobs=1,       param_grid=[{&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}],       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,       scoring=&#39;neg_mean_squared_error&#39;, verbose=0)</code></pre><pre><code class="python"># 搜索完成以后你将会得到在你给予的值范围内最好的超参数，# 因为n_estimators超参数在我们的搜索中30是最大值，我们获取可以继续增大该值，或许能获取到更优的超参数print(grid_search.best_params_) # 最好的超参数print(grid_search.best_estimator_) # 获取最好的模型cvres = grid_search.cv_results_ # 所有策略评估分数for mean_score, params in zip(cvres[&quot;mean_test_score&quot;],cvres[&quot;params&quot;]):    print(np.sqrt(-mean_score),params)</code></pre><pre><code>{&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30}RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,           min_impurity_split=None, min_samples_leaf=1,           min_samples_split=2, min_weight_fraction_leaf=0.0,           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,           verbose=0, warm_start=False)64119.85349022265 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}55381.23545742357 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}53075.44430708791 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30}60409.23914852959 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}52752.70703014053 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}50688.20150002824 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30}58807.05957827667 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3}52001.301422501005 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10}50192.68760353124 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30}58407.14908967587 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3}52082.05779305404 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10}50055.361023543235 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30}63019.801729902276 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}54226.00106549722 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}61102.60574526821 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3}52792.63987300767 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10}59220.41356163033 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}51283.813598815985 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}</code></pre><h2 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h2><p>随机搜索与网格搜索使用方法类似，sklearn中的方法 RandomizedSearchCV</p><h2 id="集成"><a href="#集成" class="headerlink" title="集成"></a>集成</h2><p>就是把比较好的几种方法进行集成，类似于随机森林</p><hr><h1 id="分析最佳模型及其错误"><a href="#分析最佳模型及其错误" class="headerlink" title="分析最佳模型及其错误"></a>分析最佳模型及其错误</h1><pre><code class="python"># 我们在进行准确预估的时候，可以指出每个属性相对的重要程度feature_importances = grid_search.best_estimator_.feature_importances_print(feature_importances)extra_attribs = [&#39;rooms_per_household&#39;,&#39;population_per_household&#39;,&#39;bedrooms_per_room&#39;]cat_one_hot_attribs = list(encoder.classes_)attributes = num_attribs + extra_attribs + cat_one_hot_attribssorted(zip(feature_importances, attributes),reverse= True)</code></pre><pre><code>[7.11837050e-02 6.49390013e-02 4.45982246e-02 1.54058658e-02 1.51343704e-02 1.51144514e-02 1.43021595e-02 3.32747738e-01 5.47807317e-02 1.11495774e-01 8.53794690e-02 4.73000789e-03 1.64498398e-01 7.99754238e-05 2.58803593e-03 3.02209250e-03][(0.3327477377793599, &#39;median_income&#39;), (0.16449839819355594, &#39;INLAND&#39;), (0.11149577356183278, &#39;population_per_household&#39;), (0.08537946903014358, &#39;bedrooms_per_room&#39;), (0.07118370498819128, &#39;longitude&#39;), (0.06493900130495682, &#39;latitude&#39;), (0.05478073171138477, &#39;rooms_per_household&#39;), (0.04459822463776366, &#39;housing_median_age&#39;), (0.015405865798022386, &#39;total_rooms&#39;), (0.015134370410352954, &#39;total_bedrooms&#39;), (0.015114451369986391, &#39;population&#39;), (0.014302159478332245, &#39;households&#39;), (0.0047300078856028485, &#39;&lt;1H OCEAN&#39;), (0.0030220924972792274, &#39;NEAR OCEAN&#39;), (0.0025880359294569487, &#39;NEAR BAY&#39;), (7.997542377816801e-05, &#39;ISLAND&#39;)]</code></pre><hr><h1 id="通过测试集评估系统"><a href="#通过测试集评估系统" class="headerlink" title="通过测试集评估系统"></a>通过测试集评估系统</h1><pre><code class="python">final_model = grid_search.best_estimator_X_test = strat_test_set.drop(&quot;median_house_value&quot;,axis = 1)y_test = strat_test_set[&quot;median_house_value&quot;].copy()x_test_prepared = full_pipeline.transform(X_test)final_predictions = final_model.predict(x_test_prepared)final_mse = mean_squared_error(y_test,final_predictions)final_rmse = np.sqrt(final_mse)final_rmse</code></pre><pre><code>47913.84534663264</code></pre><p>模型已经成型，进入项目预启动阶段，你需要展示你的解决方案（强调学习了什么，有什么用，什么没有用，基于什么假设，以及系统的限制有什么），记录所有事情，通过清晰的可视化和易于记忆的陈述方式，制作漂亮的演示文稿</p><hr><h1 id="启动、监控、维护系统"><a href="#启动、监控、维护系统" class="headerlink" title="启动、监控、维护系统"></a>启动、监控、维护系统</h1><ul><li>做好生产数据接入系统准备</li><li>编写监控代码定期检查实时性能，同时在性能下降时发出警告</li><li>还需要评估输入系统的数据质量</li><li>尽可能自动化完成一定的时间自训练模型，如果是在线的学习系统，应该定期保存系统状态，快速备份</li></ul><h1 id="START"><a href="#START" class="headerlink" title="START"></a>START</h1><p><a href="http://kaggle.com/" target="_blank" rel="noopener">http://kaggle.com/</a> 是一个不错的网站，它会给你一个数据集，一个明确的目标，同时也可一起分享经验</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> scikitlearn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> BOOK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oracle 中 varchar2 &amp; varchar </title>
      <link href="undefined2019/08/23/Oracle-%E4%B8%AD-varchar2-varchar/"/>
      <url>2019/08/23/Oracle-%E4%B8%AD-varchar2-varchar/</url>
      
        <content type="html"><![CDATA[<h1 id="直观"><a href="#直观" class="headerlink" title="直观"></a>直观</h1><p>varchar – 存放定長的字符数据，最長2000个字符；</p><p>varchar2 – 存放可变长字符数据，最大长度为4000字符。</p><h1 id="隐含"><a href="#隐含" class="headerlink" title="隐含"></a>隐含</h1><p>1.varchar2把所有字符都占两字节处理(一般情况下)，varchar只对汉字和全角等字符占两字节，数字，英文字符等都是一个字节</p><p>2.VARCHAR2把空串等同于null处理，而varchar仍按照空串处理</p><p>3.VARCHAR2字符要用几个字节存储，要看数据库使用的字符集</p><p><font color="red">注意：varchar 在获取到字符串时，若字符长度不够，会在数据后方以空格补齐保证定长， varchar2 会以原数据内容进行存储</font></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> Oracle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Pull 与 Git Fetch 的区别</title>
      <link href="undefined2019/08/23/Git-Pull-%E4%B8%8E-Git-Fetch-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>2019/08/23/Git-Pull-%E4%B8%8E-Git-Fetch-%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>说来也惭愧,这是在一场面试中,面试官口述问我的一道题。面试官的原话是这样的：你平时常用Git的话,那请问<code>Git Pull</code>和<code>Git Fetch</code>有什么区别？</p><p>当时我就蒙住了，Git在平时的使用中是挺常用的，公司平时都是小项目，对于代码安全来说并不是特别在意，平时也就用<code>Git Pull</code>直接把服务器上最新的代码给拉取到主分支，再创建分支，在分支上做开发,最后再合并。</p><p>平时也没查过fetch相关的内容，这可不，就只能栽在这了。</p><h1 id="Git-Pull"><a href="#Git-Pull" class="headerlink" title="Git Pull"></a>Git Pull</h1><p><code>Git Pull</code> 命令是Git当中将远程版本库中最新的内容拉取到本地的操作，该操作会直接将远程版本库中的内容直接拉取到本地，并将本地内容与拉取的内容做合并，在不经过任何统一的情况下，只要不遇见版本冲突等其他问题，就相当于直接替换了本地的版本内容。</p><p>用法：<code>git pull origin master</code></p><h1 id="Git-fetch"><a href="#Git-fetch" class="headerlink" title="Git fetch"></a>Git fetch</h1><p><code>Git Fetch</code> 命令是将远程版本库中的最新版本内容单纯拉取到本地的操作，拉取到本地的内容将会成为一个单独的分支，如果不进行后续的操作，主分支将不会做任何修改，也不会替换为远程版本库中的最新内容。需要通过查看是否需要当前更新的内容，然后再将此分支与本地主分支合并。</p><p>用法：<code>git fetch origin master</code>,<code>git log -p master.. origin/master</code>查看更改后，判断是否需要此项更新内容，需要则将分支进行合并 <code>git merge origin/master</code></p><h1 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h1><p>两者的功能大致相同，不同点就在于<code>git pull</code>是将版本直接拉取并合并到本地分支，而<code>git fetch</code>的操作则可以控制中间的步骤，查看更新内容，判断是否需要更新，相对于<code>git pull</code>来说，更加安全。</p><p>用何种方式对于大家来说应该是自己心里有底。<code>git pull</code> 方便，在小规模，互相信任的时候，比较适用，但在多人协作的情况下并不安全，<code>git fetch</code>对于大规模，多人协作的情况下有很大的优势。</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（三）</title>
      <link href="undefined2018/10/22/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>2018/10/22/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="管理者应该具备的管理技能"><a href="#管理者应该具备的管理技能" class="headerlink" title="管理者应该具备的管理技能"></a>管理者应该具备的管理技能</h1><p>沟通协调可行性方案</p><ol><li>执行控制</li><li>目标计划</li><li>指挥授权</li><li>培养激励</li></ol><h1 id="沟通的六个步骤"><a href="#沟通的六个步骤" class="headerlink" title="沟通的六个步骤"></a>沟通的六个步骤</h1><ol><li>事前准备</li><li>融洽关系</li><li>调查需求</li><li>阐述观点</li><li>处理异议</li><li>达成共识</li></ol><h1 id="表扬的艺术"><a href="#表扬的艺术" class="headerlink" title="表扬的艺术"></a>表扬的艺术</h1><p>高层次的表扬 ， 低层次的批评</p><p>表扬可以对人   批评需要对事</p><h1 id="沟通"><a href="#沟通" class="headerlink" title="沟通"></a>沟通</h1><p>沟通应该营造良好的氛围。当别人有想法时，应该选择倾听，适当记好笔记。对话的衔接应该多肯定，表达观点应该逻辑清晰，及时响应。沟通思想的同事也需要沟通情感</p><h1 id="沟通的方式"><a href="#沟通的方式" class="headerlink" title="沟通的方式"></a>沟通的方式</h1><p>涉及到敏感的问题，需要当面说，遇到决策性问题，应该以会议的形式，后期并以纸面形式确定会议内容。</p><p>有的沟通需要冷媒体（白纸黑字），实现有据可查</p><p>有的沟通需要热媒体（面对面的沟通），可以深入人心</p><p>有的沟通需要多媒体，重要性、准确性、多对象性</p><p><code>注意兼顾效率和效果，查找最可靠的沟通方式</code></p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（二）</title>
      <link href="undefined2018/10/16/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>2018/10/16/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="1-别人心目中理想的管理者"><a href="#1-别人心目中理想的管理者" class="headerlink" title="1.别人心目中理想的管理者"></a>1.别人心目中理想的管理者</h1><p>换位思考：发现领导眼中的我什么样的，发现下属眼中我是什么样的</p><p>从而领悟自己在领导眼中和下属眼中最应该提升什么</p><h1 id="2-层级意识"><a href="#2-层级意识" class="headerlink" title="2.层级意识"></a>2.层级意识</h1><p>建立信任、坦诚关系</p><p>站在上司角度看问题</p><p>试探上司让上司授权，提出合理化的建议，不越级指挥和汇报</p><p>及时提醒上司武略的事情</p><p>不盲从，合理“管理”上司</p><h1 id="3-管理者意识"><a href="#3-管理者意识" class="headerlink" title="3.管理者意识"></a>3.管理者意识</h1><p>①做管理者要先有 目标感 、全局感</p><p>②不漠视规则</p><p>③做事，快准狠</p><p>④不要低头做事，要学会抬头看路</p><p>⑤循规蹈矩不一定永远有效</p><p>⑥适当敏感和质疑现在的路线是必要的</p><p>⑦及时纠偏非常重要</p><h1 id="4-规矩"><a href="#4-规矩" class="headerlink" title="4.规矩"></a>4.规矩</h1><p>所有规矩，先坚决执行后，再提出相应的建议</p><h1 id="5-心态正面积极"><a href="#5-心态正面积极" class="headerlink" title="5.心态正面积极"></a>5.心态正面积极</h1><p>做企业中的发光体，不做黑洞，情绪A（成因）B（看法）C（后果）疗法，把握可控的，控制个人对时间的看法，改变事件的结果</p><h1 id="6-合作共赢"><a href="#6-合作共赢" class="headerlink" title="6.合作共赢"></a>6.合作共赢</h1><p>寻找合作中的交叉共赢点，把生活、工作、学习当做是合作方。</p><p>寻找共赢点，达到双方共赢的目的</p><h1 id="7-学历-lt-gt-学习力"><a href="#7-学历-lt-gt-学习力" class="headerlink" title="7.学历&lt;&gt;学习力"></a>7.学历&lt;&gt;学习力</h1><p>学历并不等于学习力，继续学习没有什么不好</p><h1 id="8-信任和欣赏"><a href="#8-信任和欣赏" class="headerlink" title="8.信任和欣赏"></a>8.信任和欣赏</h1><p>没人身上都有值得欣赏的地方，每个人都会向被欣赏的方向努力</p><p>当批评的方式不管用的时候可以试试使用欣赏的方式</p><p>欣赏的三要素：及时 、具体 、发自内心</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（四）</title>
      <link href="undefined2018/10/10/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>2018/10/10/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="沟通"><a href="#沟通" class="headerlink" title="沟通"></a>沟通</h1><p>沟通是以解决问题，达成一致的目标为方向的</p><p>因此，所有场合的沟通，是需要肯定对方。</p><p>所有的沟通都一肯定对方为主</p><h2 id="沟通批评的三步走法则"><a href="#沟通批评的三步走法则" class="headerlink" title="沟通批评的三步走法则"></a>沟通批评的三步走法则</h2><ul><li>肯定 先肯定对方</li><li>批评 在就事论事，高表扬 ，低批评</li><li>信任 信任对方</li></ul><h1 id="管理学"><a href="#管理学" class="headerlink" title="管理学"></a>管理学</h1><h2 id="管理的重心"><a href="#管理的重心" class="headerlink" title="管理的重心"></a>管理的重心</h2><p>管理的重心不应该是工作和物质上的因素，应该把目光转到人的因素上去，人被重视了以后，就会更加卖力，做的更加出色</p><p>目标的管理并不是管理目标，而是管理从目标到得到成果的全过程</p><h1 id="目标的制定"><a href="#目标的制定" class="headerlink" title="目标的制定"></a>目标的制定</h1><h2 id="目的目标计划"><a href="#目的目标计划" class="headerlink" title="目的目标计划"></a>目的目标计划</h2><ul><li>目的：努力的方向</li><li>目标：预期的结果</li><li>计划：为实现目标和目的，设定安排好 责任人、实施方法、时间日程</li></ul><p>做好目标，考虑做什么样的人，成就什么样的事业，才会向着自己的方向前进</p><h2 id="制定目标的好处"><a href="#制定目标的好处" class="headerlink" title="制定目标的好处"></a>制定目标的好处</h2><ul><li>目标是行动的基础、导航，为行动指明方向</li><li>有目标才能有效的利用资源，不会漫无目的</li><li>达成目标后，有成就感，有激励的作用</li></ul><h2 id="企业的绩效需要从四个方面来衡量"><a href="#企业的绩效需要从四个方面来衡量" class="headerlink" title="企业的绩效需要从四个方面来衡量"></a>企业的绩效需要从四个方面来衡量</h2><p>企业的绩效不能只从财务指标角度来衡量考虑</p><p>应该从财务、客户、内部流程、学习与成长 四个层面来衡量企业的业绩</p><h2 id="目标的制定-1"><a href="#目标的制定-1" class="headerlink" title="目标的制定"></a>目标的制定</h2><ul><li>高层基于战略通过SWOT分析指定整体目标</li><li>遵循SMART原则</li><li>目标层级清晰</li><li>目标数量3-5个</li><li>由上而下 基于事实 逐层分解 参与制定 忌过高过低 激发实现意愿</li><li>目标对话 （目标意义、预测困难、解决方案、适当授权、明确奖惩、提出支持）</li></ul><h2 id="SWOT分析"><a href="#SWOT分析" class="headerlink" title="SWOT分析"></a>SWOT分析</h2><ol><li>Strangths 优势</li><li>Weaknesses 劣势</li><li>Threats 威胁</li><li>Opportunities 机会</li></ol><p>通过分析：</p><ul><li>找出有利的、值得发扬的因素，找出不利的、要避免的因素，发现问题，找出解决方案</li><li>根据轻重缓急或影响程度等一一列出</li><li>根据战略目标的基本思路 客服各种因素</li></ul><h2 id="目标管理SMART原则"><a href="#目标管理SMART原则" class="headerlink" title="目标管理SMART原则"></a>目标管理SMART原则</h2><ul><li>明确的</li><li>可度量的</li><li>可达到的</li><li>相关联的</li><li>有时限的</li></ul><h2 id="目标结构化分解的要点"><a href="#目标结构化分解的要点" class="headerlink" title="目标结构化分解的要点"></a>目标结构化分解的要点</h2><ul><li>某项任务的内容时旗下所有任务的总和</li><li>各项任务应该区分轻重缓急</li><li>一项任务只能由一个人负责，及时许多人都可能为其工作，也只能有一个人负责，其他人只能是参与者</li><li>应该让团队成员积极参与目标分解，以确保达成共识</li><li>每项任务最终要文档化，以确保准确理解和跟踪</li><li>每项任务应该有预案和弥补措施</li><li>通常层次不超过10层建议4-6层</li></ul><h2 id="下达任务要素"><a href="#下达任务要素" class="headerlink" title="下达任务要素"></a>下达任务要素</h2><ol><li>what</li><li>why</li><li>who</li><li>when</li><li>where</li><li>how</li><li>how much</li></ol><p>下达任务：必须交代的问题 1 ，3 ，4 ，7</p><h2 id="授权与交责"><a href="#授权与交责" class="headerlink" title="授权与交责"></a>授权与交责</h2><p>谁的任务谁来管，照顾好自己的任务</p><p>不要出现没人管理的任务</p><p>管理者应该培养下属管理好自己的任务</p><p>下属应该提出选择题，而不是问题</p><p>管理者不要当“家长”、“保姆”</p><p>教练式启发，引发下属思考</p><p>及时鼓励和肯定</p><p>恰当授权，让下属担当（锁定责任人）</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTP综合管理技能提升培训（一）</title>
      <link href="undefined2018/08/06/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>2018/08/06/MTP%E7%BB%BC%E5%90%88%E7%AE%A1%E7%90%86%E6%8A%80%E8%83%BD%E6%8F%90%E5%8D%87%E5%9F%B9%E8%AE%AD%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="1-知识体系"><a href="#1-知识体系" class="headerlink" title="1.知识体系"></a>1.知识体系</h1><p>MTP综合管理技能 知识体系 ：① 管理心理学 ②九型人格 ③神经语言程序学 ④教练技术 ⑤管理培训计划</p><h1 id="2-管理技能架构"><a href="#2-管理技能架构" class="headerlink" title="2.管理技能架构"></a>2.管理技能架构</h1><p>对于自己（角色，心态）：管理好自己的角色和心态<br>对于人（培养，沟通，激励）：做好教练，教练（教练技术一项通过改善被教练者心智模式来发挥其潜能和提升效率的管理技术）<br>对于事（计划，指挥，控制）：顾问 给予做事的方法</p><h1 id="3-缺点，盲点"><a href="#3-缺点，盲点" class="headerlink" title="3.缺点，盲点"></a>3.缺点，盲点</h1><p>失败是发现自己盲点的绝佳机会，失败中自我反思失败原因</p><h1 id="4-优秀管理者的素质模型"><a href="#4-优秀管理者的素质模型" class="headerlink" title="4.优秀管理者的素质模型"></a>4.优秀管理者的素质模型</h1><p>①领袖素质（决定管理层高低）：激励 负责任 信任 共赢<br>②基本管理能力：科学决策 授权 沟通 目标方向<br>③人力资源管理能力（非人力资源管理技能）：选育用留<br>④业务管理能力:专业知识和业务管理能力</p><h1 id="5-信念"><a href="#5-信念" class="headerlink" title="5.信念"></a>5.信念</h1><p>信念决定人生高度，扩宽自己的信念，则会有更大的成果 </p><h1 id="6-素质领袖"><a href="#6-素质领袖" class="headerlink" title="6.素质领袖"></a>6.素质领袖</h1><p>发挥自己的优势，弥补自己最大的缺陷</p><h1 id="7-称职？"><a href="#7-称职？" class="headerlink" title="7.称职？"></a>7.称职？</h1><p>何为称职  人总是处于有挑战的位置，每个人的职位应为 20%的挑战 + 80%的把握  </p><p>因此，需要①包容上司处于不胜任的状态②接纳自己的不足③洞察下属是否有超胜任及超不胜任的情况</p><h1 id="8-探秘术"><a href="#8-探秘术" class="headerlink" title="8.探秘术"></a>8.探秘术</h1><p>凡事，先“横推”，后“下切”</p><h1 id="9-管理者角色的认知"><a href="#9-管理者角色的认知" class="headerlink" title="9.管理者角色的认知"></a>9.管理者角色的认知</h1><p>企业中的管理者是经营者的替身，考虑和看待问题需要从经营者的一直和利益出发，从经营者的角度看待整个问题</p><h1 id="10-着眼当前，放眼未来"><a href="#10-着眼当前，放眼未来" class="headerlink" title="10.着眼当前，放眼未来"></a>10.着眼当前，放眼未来</h1><p>数据分析当前的状态，综合现在的管理制度，优化和改善现在的制度</p>]]></content>
      
      
      <categories>
          
          <category> MTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zabbix Server 配置优化</title>
      <link href="undefined2017/09/22/Zabbix-Server-%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
      <url>2017/09/22/Zabbix-Server-%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>Zabbix 是一款性能特别高的一款分布式监控报警系统，在平时的运行维护当中，是一款不错的监控软件，可以实施监控服务器以及各种设备设施的状态。</p><h2 id="Zabbix-简单的优化调整"><a href="#Zabbix-简单的优化调整" class="headerlink" title="Zabbix 简单的优化调整"></a>Zabbix 简单的优化调整</h2><p>由于监控设备的数量是不定的，有些时候因为默认参数配置的不够，会导致Zabbix自身状态不稳定，导致报警。比如说：</p><p><code>Zabbix unreachable poller processes more than 75% busy</code></p><p><code>Zabbix discoverer processes more than 75% busy</code></p><p>因此，为了Zabbix运行的更加稳定，我们就需要对Zabbix做相应的优化处理</p><h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><p><font color="pink">这部分很重要</font></p><p>下面是我们监控项中所有项目所对应的参数配置。</p><p>关于 Zabbix data gathering process busy % 参数</p><pre><code>Zabbix busy trapper processes, in %                     StartTrappers=Zabbix busy poller processes, in %                      StartPollers=Zabbix busy ipmi poller processes, in %                 StartIPMIPollers=Zabbix busy discoverer processes, in %                  StartDiscoverers=Zabbix busy icmp pinger processes, in %                 StartPingers=Zabbix busy http poller processes, in %                 StartHTTPPollers=Zabbix busy proxy poller processes, in %                StartProxyPollers=Zabbix busy unreachable poller processes, in %          StartPollersUnreachable=Zabbix busy java poller processes, in %                 StartJavaPollers=Zabbix busy snmp trapper processes, in %                StartSNMPTrapper=Zabbix busy vmware collector processes, in %            StartVMwareCollectors=</code></pre><p>关于 Zabbix cache usage 参数</p><pre><code>Zabbix-server: Zabbix trend write cache, % free         TrendCacheSize=Zabbix-server: Zabbix configuration cache, % free       CacheSize=Zabbix-server: Zabbix text write cache, % free          HistoryTextCacheSize=Zabbix-server: Zabbix history write cache, % free       HistoryCacheSize=Zabbix-server: Zabbix value cache, % free               ValueCacheSize=Zabbix-server: Zabbix vmware cache, % free              VMwareCacheSize=</code></pre><h4 id="调整优化"><a href="#调整优化" class="headerlink" title="调整优化"></a>调整优化</h4><p>在监控的时候，Zabbix 监控出现性能瓶颈的时候会发生警报，我们可以根据警报项，对我们 Zabbix的配置文件做相应的调整，对Zabbix做相应的优化，使之适应我们需要的工作环境。</p><p>下面我们是使用一个报警参数来进行解释说明。</p><p>报警信息：Zabbix unreachable poller processes more than 75% busy </p><p>按照信息，我们可以得出，报警信息所对应的配置参数为<code>StartProxyPollers</code> ，我们配置文件中此参数所默认的参数值为 1，我们可以对此参数做相应的提高，以提高我们Zabbix的性能。</p><p>步骤如下：</p><p>1.找到我们Zabbix服务端安装路径。并在安装目录下找到服务器端配置文件 <font color="green">zabbix_server.conf</font> ，例如我的配置文件目录如下，可以参考。</p><pre><code class="bash">$ cd /usr/local/zabbix/etc/</code></pre><p>2.修改配置文件参数<font color="green">(文件名可能会有差异，差异在于中间的下划线，请勿直接拷贝)</font></p><pre><code class="bash">$ vim zabbix_server.conf</code></pre><p>在默认的<code># StartProxyPollors = 1</code>下方添加一条配置文件</p><p><code>StartProxyPollors = 5</code> <font color="green">//参数可以根据自己需要增大，寻找自己环境适合的参数值</font></p><p>3.重启zabbix服务，使刚才修改的配置文件生效。</p><pre><code class="bash">$ service zabbix_server restart</code></pre><p>回到监控系统中看看效果吧</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>